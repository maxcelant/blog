{"index":{"slug":"index","filePath":"index.md","title":"Hi There üëã","links":["www.github.com/maxcelant","www.linkedin/in/maxcelant"],"tags":[],"content":"My name is Massimiliano (Max), I currently work at American Airlines as a Platform Engineer on the Kubernetes as a Platform team.\nMore specifically, I work on the Kubernetes operators squad where we help support over 300 American Airlines applications.\nI like to dive into different tech related topics on my free time and read technical books. I‚Äôm going to use this space to expand on my\nlearning and dive into interesting topics I want to get a better grasp of.\nI‚Äôm not anti-AI, but I won‚Äôt be using it to assist my writing. I feel like that defeats the purpose of writing a blog.\nIf you are interested in following me:\n\nGithub\nLinkedin\n"},"posts/runner-orchestration-in-controller-runtime":{"slug":"posts/runner-orchestration-in-controller-runtime","filePath":"posts/runner-orchestration-in-controller-runtime.md","title":"Controller Orchestration Model in Kubernetes Controller Runtime","links":[],"tags":[],"content":"The manager handles the life-cycle of all the runnable objects created by controller-runtime. To clarify,Runnable is just a simple interface with a Start(context.Context) error function. The manager splits these runnables by functionality into runnableGroups, so that like-minded objects can be added, reconciled, and shutdown together. Each runnable spins off into it‚Äôs own goroutine so that all of the runnables can be orchestrated by the manager. The question then becomes, how exactly does the manager effectively orchestrate the lifecycle of one of these runnable groups? Let‚Äôs take a closer look:\nStep by Step\n\n\nThe Start function starts by using first encapsulating the logic in a sync.Once callback called startOnce, to ensure the inside only runs one time.\nfunc (r *runnableGroup) Start(ctx context.Context) error {\n\tvar retErr error\n \n\tr.startOnce.Do(func() {\n        ...\n\n\nWe kick off a goroutine withr.reconcile() .\n\t\tgo r.reconcile()\n\n\nWe attain the lock, mark the group as started and mark each runnable in the group as signalReady=true (this will come up later) and add it to the runnable dispatch channel.\n\t\tr.start.Lock()\n\t\tr.started = true\n\t\tfor _, rn := range r.startQueue {\n\t\t\trn.signalReady = true\n\t\t\tr.ch &lt;- rn\n\t\t}\n\t\tr.start.Unlock()\n\n\nIf there is nothing in the start queue, we simply return.\n\t\tif len(r.startQueue) == 0 {\n\t\t\treturn\n\t\t}\n\n\nThis next section involves coordination with the reconcile function, so let‚Äôs look at that first. We start by reading off of the dispatch channel which we filled in step 3.\nfunc (r *runnableGroup) reconcile() {\n\tfor runnable := range r.ch {\n\t\t...\n\n\nPart of the interesting bit to this logic is that the manager can support adding new runnables after having already started. This portion is key, we only want to add the runnable to the wait group if the manager is not in shutdown sequence. The reason is that executing wg.Add after wg.Wait is called will cause the program to panic.\nwg.Wait is a blocking call that waits for the wait group to decrement back to 0, so calling wg.Add after doesn‚Äôt make any sense‚Äîhence the panic.\nSo in the chance we are in shutdown, we simply continue and avoid adding the runner.\n\t\t{\n\t\t\tr.stop.RLock()\n\t\t\tif r.stopped {\n\t\t\t\tr.errChan &lt;- errRunnableGroupStopped\n\t\t\t\tr.stop.RUnlock()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tr.wg.Add(1)\n\t\t\tr.stop.RUnlock()\n\t\t}\n\n\nThis is the portion that actually starts the individual runner. The are some important aspects. First, you‚Äôll notice this whole area is nested in a goroutine, that‚Äôs because each runnable runs on its own. Another key is the goroutine nested within that, which is used to signal back to the Start method (I told you it would come back up!), telling it ‚ÄúHey, this runnable has officially started!‚Äú.\nLastly, we make sure to defer wg.Done() to ensure that this runnable is correctly marked off in the shutdown process (decrementing that counter we spoke of earlier).\n\t\tgo func(rn *readyRunnable) {\n            // Signal back to Start method\n\t\t\tgo func() {\n\t\t\t\tif rn.Check(r.ctx) {\n\t\t\t\t\tif rn.signalReady {\n\t\t\t\t\t\tr.startReadyCh &lt;- rn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n \n\t\t\tdefer r.wg.Done()\n \n\t\t\tif err := rn.Start(r.ctx); err != nil {\n\t\t\t\tr.errChan &lt;- err\n\t\t\t}\n\t\t}(runnable)\n\n\nGoing back to the Start method, we can see the relationship it has with reconcile.\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase &lt;-ctx.Done():\n\t\t\t\tif err := ctx.Err(); !errors.Is(err, context.Canceled) {\n\t\t\t\t\tretErr = err\n\t\t\t\t}\n            // Remove the runnable from the queue\n\t\t\tcase rn := &lt;-r.startReadyCh:\n\t\t\t\tfor i, existing := range r.startQueue {\n\t\t\t\t\tif existing == rn {\n\t\t\t\t\t\tr.startQueue = append(r.startQueue[:i], r.startQueue[i+1:]...)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif len(r.startQueue) == 0 {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n \nWe read runnables off of the start ready queue which we triggered in step 7 in it‚Äôs own goroutine. We find it and eliminate it from the start queue. Once the start queue is empty, we can finally quit out of the Start method because we‚Äôve successfully kicked off the lifecycle of all our runnables! Let‚Äôs now see how the shutdown process handles these runnables.\n\n\nShutdown is signaled through the StopAndWait method. Once again we use sync.Once to execute the section exactly once.\nfunc (r *runnableGroup) StopAndWait(ctx context.Context) {\n\tr.stopOnce.Do(func() {\n\t\t...\n\n\nWe first defer the closing of the dispatch channel, we don‚Äôt want any more runnables being added and spun up.\n\t\tdefer func() {\n\t\t\tr.stop.Lock()\n\t\t\tclose(r.ch)\n\t\t\tr.stop.Unlock()\n\t\t}()\n\n\nNext we internally call Start, this might seem backwards at first but we call it here to make sure we kick off the reconcile loop and consume the r.ch channel. We want to make sure that any runnables stuck in the queue are consumed and our wg can be reduced down to 0.\n\t\t_ = r.Start(ctx)\n\t\tr.stop.Lock()\n\t\t// Store the stopped variable so we don&#039;t accept any new\n\t\t// runnables for the time being.\n\t\tr.stopped = true\n\t\tr.stop.Unlock()\n\n\nThe context is passed into all the runnables in step 7 line 12, so by calling cancel here, we are effectively signaling to all the runnables, ‚ÄúHey guys, the shutdown process is in effect!‚Äù, which causes those loops to break.\n\t\t// Cancel the internal channel.\n\t\tr.cancel()\n\n\nHere we have an interesting concurrency pattern at play. This pattern let‚Äôs us either wait for the wait group to complete, i.e, reach 0 or wait for a set timer to expire. This is nice because it gives us flexibility in a non-blocking fashion. We don‚Äôt want to wait forever for the runnables to be cleaned up in situations where maybe some block occurred.\n\t\tdone := make(chan struct{})\n\t\tgo func() {\n\t\t\tdefer close(done)\n\t\t\tr.wg.Wait()\n\t\t}()\n \n\t\tselect {\n\t\tcase &lt;-done:\n\t\t\t// We&#039;re done, exit.\n\t\tcase &lt;-ctx.Done():\n\t\t\t// Calling context has expired, exit.\n\t\t}\n\t})\n}\n\n\nAnd there you have it! The complete controller lifecycle orchestrated by the manager in controller-runtime. Feel free to view the code in it‚Äôs entirety here."}}