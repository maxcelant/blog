{"index":{"slug":"index","filePath":"index.md","title":"Hi There üëã","links":["www.github.com/maxcelant","www.linkedin/in/maxcelant"],"tags":[],"content":"My name is Massimiliano (Max), I currently work at American Airlines as a Platform Engineer on the Kubernetes as a Platform team.\nMore specifically, I work on the Kubernetes operators squad where we help support over 300 American Airlines applications.\nI like to dive into different tech related topics on my free time and read technical books. I‚Äôm going to use this space to expand on my\nlearning and dive into interesting topics I want to get a better grasp of.\nI‚Äôm not anti-AI, but I won‚Äôt be using it to assist my writing. I feel like that defeats the purpose of writing a blog.\nIf you are interested in following me:\n\nGithub\nLinkedin\n"},"posts/runner-orchestration-in-controller-runtime":{"slug":"posts/runner-orchestration-in-controller-runtime","filePath":"posts/runner-orchestration-in-controller-runtime.md","title":"Controller Orchestration Model in Controller Runtime Library","links":[],"tags":[],"content":"The controller-runtime library is utilized by numerous controller builders and templating engines (Kubebuilder, Operator SDK to name a few) to standardize the management of Kubernetes controllers. It handles the lifecycle of these controllers as well as webhooks, caches, servers and more while offering a fairly simple interface to build your operator on top of through it‚Äôs Reconciler.\nI‚Äôve been curious about how it works internally and decided to start diving into how the Manager handles the life-cycle of all these runnables.\nTo clarify,Runnable is just a simple interface with a Start function which controllers, webhooks, caches and more all implement.\ntype Runnable interface {\n\tStart(context.Context) error\n}\nDigging a bit deeper, you‚Äôll notice that the manager splits these runnables by functionality into runnableGroups, so that like-minded objects can be added, reconciled, and shutdown together. For instance, the way webhooks are handled internally is completely different from a leader elected controller, yet the life-cycle can be handled the same‚Äîthey all need to start and stop.\nEach runnable spins off into it‚Äôs own goroutine so that all of the runnables can do their jobs at the same time. The question then becomes, how exactly does the manager effectively orchestrate the lifetime of these runnables? Let‚Äôs take a closer look:\nStep by Step\n\n\nThe Start function starts by using first encapsulating the logic in a sync.Once callback called startOnce, to ensure the inside only runs one time.\nfunc (r *runnableGroup) Start(ctx context.Context) error {\n\tvar retErr error\n \n\tr.startOnce.Do(func() {\n        ...\n\n\nWe kick off a goroutine withr.reconcile . This is the internal reconciler that initiates all of the runnables. You‚Äôll see very shortly how it works.\ngo r.reconcile()\n\n\nWe attain the lock, mark the group as started and mark each runnable in the group as ready to start (Keep signalReady in mind, it‚Äôll come up later) and add it to the runnable dispatch channel simply called ch.\nr.start.Lock()\nr.started = true\nfor _, rn := range r.startQueue {\n\trn.signalReady = true\n\tr.ch &lt;- rn\n}\nr.start.Unlock()\n\n\nIf there is nothing in the start queue, we return from Start. There‚Äôs nothing to run, so there‚Äôs nothing to do.\nif len(r.startQueue) == 0 {\n\treturn\n}\n\n\nThis next section involves coordination with the reconcile method which we recently started in it‚Äôs own goroutine in step 2, so let‚Äôs look at that method first. We start by reading off of the dispatch channel ch which we filled in step 3.\nfunc (r *runnableGroup) reconcile() {\n\tfor runnable := range r.ch {\n\t\t...\n\n\nThe first thing we do in the loop is very important. Part of the interesting bit to this logic is that the manager can support adding new runnables after having already started. With that in mind, we only want to add the runnable to the wait group if the manager is not in shutdown sequence. The reason is that executing wg.Add after wg.Wait is called will cause the program to panic.\nwg.Wait is a blocking call that waits for the wait group to decrement back to 0, so calling wg.Add after doesn‚Äôt make any sense‚Äîhence the panic.\nSo in the chance we are in shutdown, we continue and avoid adding the runner.\n{\n\tr.stop.RLock()\n\tif r.stopped {\n\t\tr.errChan &lt;- errRunnableGroupStopped\n\t\tr.stop.RUnlock()\n\t\tcontinue\n\t}\n\tr.wg.Add(1)\n\tr.stop.RUnlock()\n}\n\n\nThis is the portion that actually starts the individual runnable. There are some important aspects to look at. First, you‚Äôll notice this whole section is nested in a goroutine, that‚Äôs because each runnable runs in parallel.\nAnother interesting bit is the nested goroutine. This block acts as a signaler back to the Start method, telling it ‚ÄúHey, this runnable has started.‚Äù It‚Äôs put it it‚Äôs own thread so that it‚Äôs non-blocking to the actual start of the runnable. It signals by sending the runnable into the startReadyCh which the Start method is ready to receive from.\nLastly, we make sure to defer wg.Done() to ensure that this runnable is correctly checked off in the shutdown process (decrementing that counter we spoke of previously).\ngo func(rn *readyRunnable) {\n\t// Signal back to Start method\n\tgo func() {\n\t\tif rn.Check(r.ctx) {\n\t\t\tif rn.signalReady {\n\t\t\t\tr.startReadyCh &lt;- rn\n\t\t\t}\n\t\t}\n\t}()\n \n\tdefer r.wg.Done()\n \n\t// Start the runnable\n\tif err := rn.Start(r.ctx); err != nil {\n\t\tr.errChan &lt;- err\n\t}\n}(runnable)\n\n\nGoing back to the Start method, we can see the relationship it has with reconcile.\nfor {\n\tselect {\n\tcase &lt;-ctx.Done():\n\t\tif err := ctx.Err(); !errors.Is(err, context.Canceled) {\n\t\t\tretErr = err\n\t\t}\n\t// Remove the runnable from the queue\n\tcase rn := &lt;-r.startReadyCh:\n\t\tfor i, existing := range r.startQueue {\n\t\t\tif existing == rn {\n\t\t\t\tr.startQueue = append(r.startQueue[:i], r.startQueue[i+1:]...)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif len(r.startQueue) == 0 {\n\t\t\treturn\n\t\t}\n\t}\n}\nWe read runnables off of the startReadyCh channel which sent the runnable down the pipe in step 7. We find it and eliminate it from the startQueue. Once the start queue is empty, we can finally quit out of the Start method because we‚Äôve successfully launched all of our runnables!\nNow, let‚Äôs now see how the shutdown process handles these runnables when we decide to stop the manager.\n\n\nShutdown is signaled through the StopAndWait method. Once again we use sync.Once to execute the section exactly once.\nfunc (r *runnableGroup) StopAndWait(ctx context.Context) {\n\tr.stopOnce.Do(func() {\n\t\t...\n\n\nWe first defer the closing of the dispatch ch channel, we don‚Äôt want any more runnables being added and spun up during the termination process.\ndefer func() {\n\tr.stop.Lock()\n\tclose(r.ch)\n\tr.stop.Unlock()\n}()\n\n\nNext we internally call Start, this might seem backwards at first, but we trigger it here to ensure we kick off the reconcile loop and consume the r.ch channel. We want to clear any runnables in the queue and ensure the wait group is back to 0.\n_ = r.Start(ctx)\nr.stop.Lock()\n// Store the stopped variable so we don&#039;t accept any new\n// runnables for the time being.\nr.stopped = true\nr.stop.Unlock()\n\n\nThe context is passed into all the runnables in step 7 line 12, so by calling cancel here, we are effectively signaling to all the runnables, ‚ÄúThe shutdown process is in effect, please stop your reconcile loops‚Äù, which causes those runnable loops to break and exit.\n// Cancel the internal channel.\nr.cancel()\n\n\nHere we have an interesting concurrency pattern at play. This pattern let‚Äôs us either wait for the wait group to complete, i.e, reach 0 or wait for a set timer to expire. This pattern is ideal because it achieves flexibility by creating multiple exit conditions.\ndone := make(chan struct{})\ngo func() {\n\tdefer close(done)\n\tr.wg.Wait()\n}()\n \nselect {\ncase &lt;-done:\n\t// We&#039;re done, exit.\ncase &lt;-ctx.Done():\n\t// Calling context has expired, exit.\n}\n\n\nAnd there you have it! The complete controller lifecycle orchestrated by the manager in controller-runtime. Feel free to view the code in it‚Äôs entirety here."},"books/100-go-mistakes-book":{"slug":"books/100-go-mistakes-book","filePath":"books/100-go-mistakes-book.md","title":"100 Go Mistakes and How to Avoid Them","links":[],"tags":[],"content":"21. Incorrectly initializing slices\nIf you know the length of your new slice, then you should set an initial capacity for it. If the allocation of the new slice is conditional, it‚Äôs up to you whether you want to use capacity.\n22. Being confused about nil vs. empty slices\nA nil slice requires no allocation if you‚Äôre not sure about the length of resulting slice or whether there are any elements at all then you should make it a nil slice. If you do know the length then you should use make.\nvar s []string   // nil slice\nvar s []string{} // empty slice\n31. Ignoring how arguments are evaluated in range loops\nWhen using a range-based for loop, it‚Äôll make a copy of the type at the beginning of the loop. However, if you use a classic for loop, then the expression is evaluated on every iteration.\n32. Ignoring the impact of using pointer elements in range loops\nFor a range-based for loop that uses pointers, the loop variable is reused on each iteration, meaning it holds a new value each iteration but keeps the same memory address.\nIf you try to store the address in a slice or a map, all the stored pointers will reference the same final element.\n33. Making wrong assumptions during map iterations\nIf you are adding entries to a map while iterating, they may or may not appear during that iteration. Create a copy of the map, so you can use one to iterate and one to update.\n34. Ignoring how break statements work\nIf you have a switch statement in a for loop with a break, it‚Äôll break the switch but not the for loop. To fix this you can use labels.\nloop:\n  for i := 0; i &lt; 5; i++ { \n    fmt.Printf(&quot;%d &quot;, i) \n    switch i { \n      default:\n      case 2:\n        break loop \n    } \n  }\n35. Using defer in loops\ndefer only executes after function is ended so if you put in a loop it will never trigger, causing leaks. The better approach is to create a function that triggers on every iteration of the loop that calls defer at the end.\n36. Not understanding how runes work\nThe len built-in function applied on a string doesn‚Äôt return the number of characters; it returns the number of bytes. We can compose a string with an array of bytes\ns := string([]byte{0xE6, 0xB1, 0x89}) \nfmt.Printf(&quot;%s\\n&quot;, s) // prints Ê±â\n38. Misusing trim functions\nTrimRight uses a set of values to determine removal. TrimSuffix just removes the given string from the end. Trim removes the given set of letters from both sides.\n39. Under optimized string concatenation\nDon‚Äôt use += to build your strings, use strings.Builder instead.\n40. Useless string conversions\nMost I/O is done with []byte, not strings. Don‚Äôt do unnecessary conversions by using strings. Most functions available in the strings package are also available in the bytes package.\n41. Strings and memory leak\nCreating a substring with slice syntax will create a copy of the entire backing string which can cause large strings to be copied in memory.\n42. Not knowing which type of receiver to use\nUse a pointer receiver if you know you‚Äôre gonna mutate the receiver or if it contains a field that cannot be copied or if it‚Äôs a large object. Use a value receiver if you want to enforce immutability, the receiver type is a map, function, or channel, or the receiver is a slice that doesn‚Äôt need to be mutated, or it‚Äôs a primitive type.\n43. Named Return Values\nGood for interfaces and short functions. Should be avoided in long functions because the empty return obscured readability\n45. Returning a Nil Receivers\nNil pointers are valid receivers because receivers are just syntax sugar on top of the first type in the argument list. In the example that we return an interface from a function, even though we return a nil struct, the interface will be not nil because it‚Äôs a wrapper around the struct. And the struct is something, it‚Äôs not nothing, even though the value is nil.\n46. Using a filename as a function input\nInstead of passing in a file name for a function that does some read operation, pass in io.Reader interface instead because that allows you to abstract the function and use it for files, HTTP requests, and much more. It also makes testing a lot easier.\n47. Ignoring how defer arguments and receivers are evaluated\nInputs of a defer function are evaluated immediately upon the function being seen, not when it actually executes. So to ensure you have the correct value, whenever it executes, you should use either closure or use a pointer to the value.\ndefer acts differently whether you are using a pointer receiver or just a plain receiver. With a pointer receiver, it‚Äôll get the most up-to-date value at the end of the functions execution, but as for the normal receiver, it‚Äôll get whatever the value is evaluated at when the defer function is called.\n50. Checking an error type incorrectly\nUse errors.As to check if the type of a wrapped error exists because using a switch statement on an typed error that‚Äôs wrapped won‚Äôt work.\nNote: errors.As is to find a type (custom struct). errors.Is is to find a error value (sentinel).\n51. Checking an error value inaccurately\nSentinel errors are created using errors.New and are used for known errors. Think of something like ErrFileNotFound.\nSentinel errors are not meant to be wrapped, because it‚Äôll obscure the type if you do so. If you do wrap it, you can still find it nested by using errors.Is.\n52. Handling errors twice\nUse wrapping to add information to errors before returning.\n53. Not handling an error\nDo _ = f() if you decide to not handle an error.\n54. Not handling defer errors\nYou can handle defer errors by using named return values.\n55. Understanding the difference between concurrency and parallelism\nConcurrency provides a structure to solve a problem with parts that may be parallelized. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. Concurrency is about structure, and we can change a sequential implementation into a concurrent one by introducing different steps that separate concurrent threads can tackle.\n56. Thinking concurrency is always faster\nOS Threads are switched on and off CPU cores, Goroutines are switched on and off OS thread by the Go runtime. Go scheduler uses GMP (Goroutine, Machine {OS thread}, Processor {core}) terminology.\nGoroutine runs a OS thread which is assigned to a CPU core.GOMAXPROCS is the max OS threads used to run user-level code.\nSo if we have 4 cores, then our goroutines will be scheduled among 4 OS threads using the Go scheduler. Go scheduler uses a global queue to assign goroutines to threads. There‚Äôs also a local queue for each processor. Goroutines aren‚Äôt efficient for handling minimal workloads.\n57. Being puzzled about when to use mutexes and channels\nChannels should be used for concurrent goroutines. Mutexes are used for parallel goroutines. Parallel goroutines usually share and mutate a shared resource, which they need to lock to ensure safe changes (via mutexes). Concurrent goroutines usually involves transferring ownership of some resource from one step to another (via channels).\n58. Not understanding race problems\nData race is when two threads try update the same memory location at the same time. Also if one goroutine is writing and the other is reading, that‚Äôs still a data race. An atomic operation can‚Äôt be interrupted, thus preventing two accesses at the same time.\nAnother option is to communicate through a channel. Using mutex is another option. A race condition occurs when the behavior depends on the sequence or the timing of events that can‚Äôt be controlled. Channels solve this by orchestrating and coordinating the order things should occur.\n59. Not understanding the concurrency impacts of a workload type\nIf the workload is CPU-bound, a best practice is to rely on GOMAXPROCS. GOMAXPROCS is a variable that sets the number of OS threads allocated to running goroutines.\nThe go scheduler is lazy and efficient. It avoids uselessly starting up OS threads if it can because that introduces latency. If a goroutine is blocking (for I/O) or CPU intensive, then it may start a new OS thread, but otherwise it‚Äôll avoid it and use its context stealing model.\n60. Misunderstanding Go contexts\nUsing context as a deadline. context.WithTimeout(ctx, time) returns a context, cancel. Behind the scenes, context spins up a goroutine that will be retained in memory for time seconds or until cancel is called.\nContext is used to send a cancellation signal to a goroutine by using WithCancel. When the main calls the cancel func is called, it triggers the ending of the goroutine using the context.\nUsing context.WithValue, you can pass down information to handlers and functions without explicitly creating variables. This can be good for trace ids.\nWhen you close a channel, it immediately unblocks. An open channel with no value will block until it gets some content it can do something with. In a buffered channel, sending is blocked when its full and receiving is blocked when its empty.\nctx.Done() unblocks when it‚Äôs closed, which acts as a signal when the channel is cancelled.\nWe can use ctx.Err() to see what the reason for the cancellation was.\nIn general, a function that users wait for should take a context, as doing so allows upstream callers to decide when calling this function should be aborted.\n61. Propagating an inappropriate context\nWe should be cautious about propagating the context because if the context is cancelled in the parent goroutine, it‚Äôll also be cancelled for the child even in scenarios where the child may have not finished its task yet!\nIn most cases, creating a new context is preferred. Keep in mind, you can also create your own context wrapper to have the functionality you want because Context is an interface.\n62. Starting a goroutine without knowing how to stop it\nStarting a goroutine without knowing when to stop it is a design issue. Using a context to cancel a goroutine didn‚Äôt necessarily mean that the parent goroutine won‚Äôt finish up before the child goroutine is fine cleaning up it‚Äôs resources.\nHaving some way to defer the clean up of the child goroutine when the parent is ending is best practice.\n63. Mishandling goroutines with loop variables.\nNote: this may be deprecated now.\nIf you spin up a goroutine in a loop and use a closured iterable from outside, the value at time of goroutine execution was non-deterministic. We can fix this by passing the variable into the goroutine as a param.\n64. Expecting deterministic behavior with select and channels\nIf multiple communications in a for-select statement can succeed, they are processed in a random order. This is to avoid starvation. where one channel has way more messages than another so the other channels are not able to receive.\nfor-select statements will run indefinitely until you explicitly break out of them.\nIn cases where you‚Äôre receiving from multiple channels in a for-select and one of them is a disconnect channel, you might want a inner for select statement for receiving the rest of the messages before closing. you would use a default case once all the messages are handled to break out.\nWe must remember that if multiple options are possible, the first case in the source order does not automatically win.\nfunc main() {\n\tmsgCh := make(chan int, 5)\n\tcancelCh := make(chan any)\n \n\tgo func() {\n\t\tfor i := range 10 {\n\t\t\tmsgCh &lt;- i\n\t\t}\n\t\tcancelCh &lt;- struct{}{}\n\t}()\n \nLoop:\n\tfor {\n\t\tselect {\n\t\tcase v := &lt;-msgCh:\n\t\t\tfmt.Println(v)\n\t\tcase &lt;-cancelCh:\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase v := &lt;-msgCh:\n\t\t\t\t\tfmt.Println(&quot;closing out &quot;, v)\n\t\t\t\tdefault:\n\t\t\t\t\tbreak Loop\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n \n65. Not using notification channels\nIf you want to convey a signal or notification through a channel, use a chan struct{} , which sends no data.\n66. Not using nil channels\nClosing a channel is non-blocking.  You can set a channel to nil and not have to worry about wasting resources reading from a channel that is closed. Good for scenarios in which you are reading from two channels simultaneously, each with their own varying length. You can see if a channel is open or closed with its second return value:v, open := &lt;- ch.\n67. Being puzzled about channel size\nUnbuffered channels (also called synchronous channels) will block the send until the receive end is ready.\nBuffered channels are harder to work with, since the queue size is unblocking until it reaches full capacity, sometimes leading to weird deadlock issues. You should think deeply about your buffered channel capacity, start with a value of 1.\nA good example is the worker pool, where goroutines need to send data to a shared channel. In that case, make the capacity equal to the threads.\nQueues are typically always close to full or close to empty due to the differences in pace between consumers and producers. They very rarely operate in a balanced middle ground where the rate of production and consumption is evenly matched.\n68. Forgetting about possible side effects with string formatting\nThe following issues are only important in concurrent programs.\nContext can hold values with WithValues. Those values can be pointers to structs, which means they can be mutated after being added.  Doing a fmt.Sprintf(&quot;%v&quot;, ctx) will recursively traverse the object and print it‚Äôs contents.\nIf we mutate the struct during the Sprintf, this will cause a data race to trigger. We are reading and writing the same memory concurrently without synchronization ‚Äî which is undefined behavior in Go.\nSure. Here‚Äôs the content reformatted into paragraph form, keeping all the original meaning intact:\n69. Creating data races with append\nWhen you hit the capacity of a slice, the backing array is doubled in size and the elements are copied over. If you try to append to a slice on two goroutines simultaneously, it creates a race condition if there is remaining length, such as with make([]int, 0, 1). Since there‚Äôs one spot remaining, both goroutines trying to append leads to a race condition. However, make([]int, 1) avoids this because any append triggers a new backing array. If you want to add an element to an existing slice and then work with it in a goroutine, make a copy of the original using copy.\n70. Using mutexes inaccurately with slices and maps\nShallow copies of slices and maps can cause data races when you create a copy of that data structure and modify it while another goroutine reads it. This is due to shared backing data structures. The correct approach is to mutex-lock the entire function interacting with that data structure or create a deep copy to prevent race conditions.\n71. Misusing sync.WaitGroup\nwg.Wait() blocks until the counter reaches 0. A common mistake is calling wg.Add inside the child goroutine, which is unsafe since there‚Äôs no guarantee it will be added before reaching wg.Wait in the parent. Always call wg.Add in the parent goroutine before launching children.\n72. Forgetting about sync.Cond\nWhen multiple goroutines receive from the same channel, only one will get the value. Channels are good for many-to-one communication, not one-to-many. In scenarios where one goroutine sends events to many listeners, sync.Cond is useful. cond.Wait() blocks until a cond.Broadcast() awakens it, and it also synchronizes with a shared mutex among listeners and the sender.\n73. Not using errgroup\nerrgroup is great for parallel tasks where you want to catch and return the first error. You use it by spinning up goroutines with g.Go(fn) and calling err := g.Wait() to wait for completion or error. Functions launched via g.Go must be context-aware; otherwise, cancelling the context won‚Äôt stop them.\n74. Copying the sync types\nNever use value receivers with sync primitives like sync.Mutex. Copying these types (e.g., by using value receivers or assigning structs directly) causes data races. Instead, use pointer receivers or make the mutex itself a pointer to ensure shared access.\n75. Providing wrong time duration\nAlways provide an int64 type alongside a time.Duration. Since time.Duration represents nanoseconds, writing something like 1000 * time.Second ensures the correct duration. Misunderstanding this can lead to incorrect timing behavior.\n76. time.After and memory leaks\nDon‚Äôt use time.After() inside a for-select loop‚Äîit causes memory leaks by creating new channels each loop iteration without releasing them. Instead, use time.Timer and call t.Reset() at the start of each loop iteration or use a context with timeout. Be cautious: timers leak resources until they expire or are garbage collected.\n77. Common JSON handling mistakes\nThe json.Marshaler interface allows structs to customize marshalling. When you embed a type that implements it, the wrapper also implements it, which may cause unexpected behavior. Also, time.Time includes wall clock and monotonic time. When unmarshalled, only the wall clock is restored. This mismatch means the structs differ pre- and post-marshalling. Use time.Equal to compare them safely.\n79. Not closing transient resources\nAlways close http.Response.Body, which implements ReadCloser, using defer to avoid memory leaks. No need for an if resp != nil check‚Äîjust ensure you only defer the close if there was no error during the request. Any type implementing io.Closer should be closed properly.\n91. Not understanding CPU caches\nModern CPUs have three cache levels: L1, L2, and L3, each larger and slower than the last. L3 is around 10√ó slower than L1. CPUs have physical and logical cores‚Äîlogical cores share parts like FPUs but not ALUs, akin to chefs sharing a kitchen. A cache line is 64 bytes (e.g., 8 ints), so storing data contiguously improves access speed.\nHow your code accesses memory (striding) matters: unit stride (e.g., arrays) is fastest, constant stride is predictable but slower, and non-unit (e.g., linked lists) is worst. Caches are partitioned; for example, a 2-way set-associative cache with 512 blocks has 256 sets. Placement is based on memory address, with a set index, tag bits, and block offset determining cache behavior. Poor striding can lead to only one set being used, harming performance through conflict misses.\n92. Writing concurrent code that leads to false sharing\nWhen two goroutines access variables in the same cache line and at least one writes, it causes false sharing. Cores mistakenly think they‚Äôre sharing data, leading to unnecessary cache invalidation and coherence traffic, reducing performance even though no logical sharing exists.\n93. Not taking into account instruction level parallelism\nInstruction Level Parallelism (ILP) allows CPUs to execute independent instructions in parallel. Hazards occur when instructions depend on each other‚Äîlike C = A + B; E = C + D‚Äîwhich must execute sequentially. Structuring code to reduce such dependencies allows the CPU to better utilize ILP.\n94. Not being aware of data alignment\nA word is 8 bytes, and CPUs prefer aligned access to word-sized chunks. Compilers insert padding to maintain alignment. For example, a 1-byte variable followed by an 8-byte one results in 7 bytes of padding. Organize struct fields to minimize padding‚Äîthis optimizes for cache locality and GC efficiency.\n95. Not understanding stack vs heap\nThe stack allocates memory at compile time and is goroutine-local. If a function returns a local variable‚Äôs address, the variable escapes to the heap. The compiler uses escape analysis to decide where to allocate. ‚ÄúSharing down‚Äù (passing to children) usually stays on the stack, while ‚Äúsharing up‚Äù (returned or referenced after) forces heap allocation.\n96. Not knowing how to reduce allocations\nUse -gcflags to inspect escape analysis decisions. Design APIs to favor sharing down. For example, io.Read lets the caller reuse buffers, avoiding heap allocations. Also, inlining simple expressions helps avoid heap allocations‚Äîlike string(foo) used inline. Use sync.Pool to reuse objects across goroutines when allocation cost is high.\n97. Not relying on inlining\nGo inlines functions that fall below a complexity threshold. Mid-stack inlining enables the compiler to optimize deeper call stacks. Inlining removes call overhead and can improve escape analysis. You can improve performance by isolating slow paths into separate functions and letting hot paths benefit from inlining.\n98. Not using Go diagnostic tools\n$ go tool pprof -http=:8080 &lt;file&gt;\n\n99. Not understanding how the garbage collector works\nThe garbage collector runs concurrently with the application and causes some‚Äùstop the world‚Äù moments that will impact the performance of your code.\nUsually the GC will run every time a certain heap threshold is reached.  GOGC is set to 100, which is 128 MB, and it can be updated. If your application has high spikes of traffic, it might be worth messing with this value."}}