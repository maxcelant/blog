{"index":{"slug":"index","filePath":"index.md","title":"Hi, I'm Max👋","links":["www.github.com/maxcelant","www.linkedin/in/maxcelant"],"tags":[],"content":"My name is Massimiliano (Max), I currently work at American Airlines as a Platform Engineer on the Kubernetes as a Platform team.\nMore specifically, I work on the Kubernetes operators squad where we help support over 300 American Airlines applications.\nI like to dive into different tech related topics on my free time and read technical books. I’m going to use this space to expand on my\nlearning and dive into interesting topics I want to get a better grasp of.\nI’m not anti-AI, but I won’t be using it to assist my writing. I feel like that defeats the purpose of writing a blog.\nIf you are interested in following me:\n\nGithub\nLinkedin\n"},"books/100-go-mistakes-book":{"slug":"books/100-go-mistakes-book","filePath":"books/100-go-mistakes-book.md","title":"100 Go Mistakes and How to Avoid Them","links":[],"tags":[],"content":"21. Incorrectly initializing slices\nIf you know the length of your new slice, then you should set an initial capacity for it. If the allocation of the new slice is conditional, it’s up to you whether you want to use capacity.\n22. Being confused about nil vs. empty slices\nA nil slice requires no allocation if you’re not sure about the length of resulting slice or whether there are any elements at all then you should make it a nil slice. If you do know the length then you should use make.\nvar s []string   // nil slice\nvar s []string{} // empty slice\n31. Ignoring how arguments are evaluated in range loops\nWhen using a range-based for loop, it’ll make a copy of the type at the beginning of the loop. However, if you use a classic for loop, then the expression is evaluated on every iteration.\n32. Ignoring the impact of using pointer elements in range loops\nFor a range-based for loop that uses pointers, the loop variable is reused on each iteration, meaning it holds a new value each iteration but keeps the same memory address.\nIf you try to store the address in a slice or a map, all the stored pointers will reference the same final element.\n33. Making wrong assumptions during map iterations\nIf you are adding entries to a map while iterating, they may or may not appear during that iteration. Create a copy of the map, so you can use one to iterate and one to update.\n34. Ignoring how break statements work\nIf you have a switch statement in a for loop with a break, it’ll break the switch but not the for loop. To fix this you can use labels.\nloop:\n  for i := 0; i &lt; 5; i++ { \n    fmt.Printf(&quot;%d &quot;, i) \n    switch i { \n      default:\n      case 2:\n        break loop \n    } \n  }\n35. Using defer in loops\ndefer only executes after function is ended so if you put in a loop it will never trigger, causing leaks. The better approach is to create a function that triggers on every iteration of the loop that calls defer at the end.\n36. Not understanding how runes work\nThe len built-in function applied on a string doesn’t return the number of characters; it returns the number of bytes. We can compose a string with an array of bytes\ns := string([]byte{0xE6, 0xB1, 0x89}) \nfmt.Printf(&quot;%s\\n&quot;, s) // prints 汉\n38. Misusing trim functions\nTrimRight uses a set of values to determine removal. TrimSuffix just removes the given string from the end. Trim removes the given set of letters from both sides.\n39. Under optimized string concatenation\nDon’t use += to build your strings, use strings.Builder instead.\n40. Useless string conversions\nMost I/O is done with []byte, not strings. Don’t do unnecessary conversions by using strings. Most functions available in the strings package are also available in the bytes package.\n41. Strings and memory leak\nCreating a substring with slice syntax will create a copy of the entire backing string which can cause large strings to be copied in memory.\n42. Not knowing which type of receiver to use\nUse a pointer receiver if you know you’re gonna mutate the receiver or if it contains a field that cannot be copied or if it’s a large object. Use a value receiver if you want to enforce immutability, the receiver type is a map, function, or channel, or the receiver is a slice that doesn’t need to be mutated, or it’s a primitive type.\n43. Named Return Values\nGood for interfaces and short functions. Should be avoided in long functions because the empty return obscured readability\n45. Returning a Nil Receivers\nNil pointers are valid receivers because receivers are just syntax sugar on top of the first type in the argument list. In the example that we return an interface from a function, even though we return a nil struct, the interface will be not nil because it’s a wrapper around the struct. And the struct is something, it’s not nothing, even though the value is nil.\n46. Using a filename as a function input\nInstead of passing in a file name for a function that does some read operation, pass in io.Reader interface instead because that allows you to abstract the function and use it for files, HTTP requests, and much more. It also makes testing a lot easier.\n47. Ignoring how defer arguments and receivers are evaluated\nInputs of a defer function are evaluated immediately upon the function being seen, not when it actually executes. So to ensure you have the correct value, whenever it executes, you should use either closure or use a pointer to the value.\ndefer acts differently whether you are using a pointer receiver or just a plain receiver. With a pointer receiver, it’ll get the most up-to-date value at the end of the functions execution, but as for the normal receiver, it’ll get whatever the value is evaluated at when the defer function is called.\n50. Checking an error type incorrectly\nUse errors.As to check if the type of a wrapped error exists because using a switch statement on an typed error that’s wrapped won’t work.\nNote: errors.As is to find a type (custom struct). errors.Is is to find a error value (sentinel).\n51. Checking an error value inaccurately\nSentinel errors are created using errors.New and are used for known errors. Think of something like ErrFileNotFound.\nSentinel errors are not meant to be wrapped, because it’ll obscure the type if you do so. If you do wrap it, you can still find it nested by using errors.Is.\n52. Handling errors twice\nUse wrapping to add information to errors before returning.\n53. Not handling an error\nDo _ = f() if you decide to not handle an error.\n54. Not handling defer errors\nYou can handle defer errors by using named return values.\n55. Understanding the difference between concurrency and parallelism\nConcurrency provides a structure to solve a problem with parts that may be parallelized. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. Concurrency is about structure, and we can change a sequential implementation into a concurrent one by introducing different steps that separate concurrent threads can tackle.\n56. Thinking concurrency is always faster\nOS Threads are switched on and off CPU cores, Goroutines are switched on and off OS thread by the Go runtime. Go scheduler uses GMP (Goroutine, Machine {OS thread}, Processor {core}) terminology.\nGoroutine runs a OS thread which is assigned to a CPU core.GOMAXPROCS is the max OS threads used to run user-level code.\nSo if we have 4 cores, then our goroutines will be scheduled among 4 OS threads using the Go scheduler. Go scheduler uses a global queue to assign goroutines to threads. There’s also a local queue for each processor. Goroutines aren’t efficient for handling minimal workloads.\n57. Being puzzled about when to use mutexes and channels\nChannels should be used for concurrent goroutines. Mutexes are used for parallel goroutines. Parallel goroutines usually share and mutate a shared resource, which they need to lock to ensure safe changes (via mutexes). Concurrent goroutines usually involves transferring ownership of some resource from one step to another (via channels).\n58. Not understanding race problems\nData race is when two threads try update the same memory location at the same time. Also if one goroutine is writing and the other is reading, that’s still a data race. An atomic operation can’t be interrupted, thus preventing two accesses at the same time.\nAnother option is to communicate through a channel. Using mutex is another option. A race condition occurs when the behavior depends on the sequence or the timing of events that can’t be controlled. Channels solve this by orchestrating and coordinating the order things should occur.\n59. Not understanding the concurrency impacts of a workload type\nIf the workload is CPU-bound, a best practice is to rely on GOMAXPROCS. GOMAXPROCS is a variable that sets the number of OS threads allocated to running goroutines.\nThe go scheduler is lazy and efficient. It avoids uselessly starting up OS threads if it can because that introduces latency. If a goroutine is blocking (for I/O) or CPU intensive, then it may start a new OS thread, but otherwise it’ll avoid it and use its context stealing model.\n60. Misunderstanding Go contexts\nUsing context as a deadline. context.WithTimeout(ctx, time) returns a context, cancel. Behind the scenes, context spins up a goroutine that will be retained in memory for time seconds or until cancel is called.\nContext is used to send a cancellation signal to a goroutine by using WithCancel. When the main calls the cancel func is called, it triggers the ending of the goroutine using the context.\nUsing context.WithValue, you can pass down information to handlers and functions without explicitly creating variables. This can be good for trace ids.\nWhen you close a channel, it immediately unblocks. An open channel with no value will block until it gets some content it can do something with. In a buffered channel, sending is blocked when its full and receiving is blocked when its empty.\nctx.Done() unblocks when it’s closed, which acts as a signal when the channel is cancelled.\nWe can use ctx.Err() to see what the reason for the cancellation was.\nIn general, a function that users wait for should take a context, as doing so allows upstream callers to decide when calling this function should be aborted.\n61. Propagating an inappropriate context\nWe should be cautious about propagating the context because if the context is cancelled in the parent goroutine, it’ll also be cancelled for the child even in scenarios where the child may have not finished its task yet!\nIn most cases, creating a new context is preferred. Keep in mind, you can also create your own context wrapper to have the functionality you want because Context is an interface.\n62. Starting a goroutine without knowing how to stop it\nStarting a goroutine without knowing when to stop it is a design issue. Using a context to cancel a goroutine didn’t necessarily mean that the parent goroutine won’t finish up before the child goroutine is fine cleaning up it’s resources.\nHaving some way to defer the clean up of the child goroutine when the parent is ending is best practice.\n63. Mishandling goroutines with loop variables.\nNote: this may be deprecated now.\nIf you spin up a goroutine in a loop and use a closured iterable from outside, the value at time of goroutine execution was non-deterministic. We can fix this by passing the variable into the goroutine as a param.\n64. Expecting deterministic behavior with select and channels\nIf multiple communications in a for-select statement can succeed, they are processed in a random order. This is to avoid starvation. where one channel has way more messages than another so the other channels are not able to receive.\nfor-select statements will run indefinitely until you explicitly break out of them.\nIn cases where you’re receiving from multiple channels in a for-select and one of them is a disconnect channel, you might want a inner for select statement for receiving the rest of the messages before closing. you would use a default case once all the messages are handled to break out.\nWe must remember that if multiple options are possible, the first case in the source order does not automatically win.\nfunc main() {\n\tmsgCh := make(chan int, 5)\n\tcancelCh := make(chan any)\n \n\tgo func() {\n\t\tfor i := range 10 {\n\t\t\tmsgCh &lt;- i\n\t\t}\n\t\tcancelCh &lt;- struct{}{}\n\t}()\n \nLoop:\n\tfor {\n\t\tselect {\n\t\tcase v := &lt;-msgCh:\n\t\t\tfmt.Println(v)\n\t\tcase &lt;-cancelCh:\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase v := &lt;-msgCh:\n\t\t\t\t\tfmt.Println(&quot;closing out &quot;, v)\n\t\t\t\tdefault:\n\t\t\t\t\tbreak Loop\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n \n65. Not using notification channels\nIf you want to convey a signal or notification through a channel, use a chan struct{} , which sends no data.\n66. Not using nil channels\nClosing a channel is non-blocking.  You can set a channel to nil and not have to worry about wasting resources reading from a channel that is closed. Good for scenarios in which you are reading from two channels simultaneously, each with their own varying length. You can see if a channel is open or closed with its second return value:v, open := &lt;- ch.\n67. Being puzzled about channel size\nUnbuffered channels (also called synchronous channels) will block the send until the receive end is ready.\nBuffered channels are harder to work with, since the queue size is unblocking until it reaches full capacity, sometimes leading to weird deadlock issues. You should think deeply about your buffered channel capacity, start with a value of 1.\nA good example is the worker pool, where goroutines need to send data to a shared channel. In that case, make the capacity equal to the threads.\nQueues are typically always close to full or close to empty due to the differences in pace between consumers and producers. They very rarely operate in a balanced middle ground where the rate of production and consumption is evenly matched.\n68. Forgetting about possible side effects with string formatting\nThe following issues are only important in concurrent programs.\nContext can hold values with WithValues. Those values can be pointers to structs, which means they can be mutated after being added.  Doing a fmt.Sprintf(&quot;%v&quot;, ctx) will recursively traverse the object and print it’s contents.\nIf we mutate the struct during the Sprintf, this will cause a data race to trigger. We are reading and writing the same memory concurrently without synchronization — which is undefined behavior in Go.\nSure. Here’s the content reformatted into paragraph form, keeping all the original meaning intact:\n69. Creating data races with append\nWhen you hit the capacity of a slice, the backing array is doubled in size and the elements are copied over. If you try to append to a slice on two goroutines simultaneously, it creates a race condition if there is remaining length, such as with make([]int, 0, 1). Since there’s one spot remaining, both goroutines trying to append leads to a race condition. However, make([]int, 1) avoids this because any append triggers a new backing array. If you want to add an element to an existing slice and then work with it in a goroutine, make a copy of the original using copy.\n70. Using mutexes inaccurately with slices and maps\nShallow copies of slices and maps can cause data races when you create a copy of that data structure and modify it while another goroutine reads it. This is due to shared backing data structures. The correct approach is to mutex-lock the entire function interacting with that data structure or create a deep copy to prevent race conditions.\n71. Misusing sync.WaitGroup\nwg.Wait() blocks until the counter reaches 0. A common mistake is calling wg.Add inside the child goroutine, which is unsafe since there’s no guarantee it will be added before reaching wg.Wait in the parent. Always call wg.Add in the parent goroutine before launching children.\n72. Forgetting about sync.Cond\nWhen multiple goroutines receive from the same channel, only one will get the value. Channels are good for many-to-one communication, not one-to-many. In scenarios where one goroutine sends events to many listeners, sync.Cond is useful. cond.Wait() blocks until a cond.Broadcast() awakens it, and it also synchronizes with a shared mutex among listeners and the sender.\n73. Not using errgroup\nerrgroup is great for parallel tasks where you want to catch and return the first error. You use it by spinning up goroutines with g.Go(fn) and calling err := g.Wait() to wait for completion or error. Functions launched via g.Go must be context-aware; otherwise, cancelling the context won’t stop them.\n74. Copying the sync types\nNever use value receivers with sync primitives like sync.Mutex. Copying these types (e.g., by using value receivers or assigning structs directly) causes data races. Instead, use pointer receivers or make the mutex itself a pointer to ensure shared access.\n75. Providing wrong time duration\nAlways provide an int64 type alongside a time.Duration. Since time.Duration represents nanoseconds, writing something like 1000 * time.Second ensures the correct duration. Misunderstanding this can lead to incorrect timing behavior.\n76. time.After and memory leaks\nDon’t use time.After() inside a for-select loop—it causes memory leaks by creating new channels each loop iteration without releasing them. Instead, use time.Timer and call t.Reset() at the start of each loop iteration or use a context with timeout. Be cautious: timers leak resources until they expire or are garbage collected.\n77. Common JSON handling mistakes\nThe json.Marshaler interface allows structs to customize marshalling. When you embed a type that implements it, the wrapper also implements it, which may cause unexpected behavior. Also, time.Time includes wall clock and monotonic time. When unmarshalled, only the wall clock is restored. This mismatch means the structs differ pre- and post-marshalling. Use time.Equal to compare them safely.\n79. Not closing transient resources\nAlways close http.Response.Body, which implements ReadCloser, using defer to avoid memory leaks. No need for an if resp != nil check—just ensure you only defer the close if there was no error during the request. Any type implementing io.Closer should be closed properly.\n91. Not understanding CPU caches\nModern CPUs have three cache levels: L1, L2, and L3, each larger and slower than the last. L3 is around 10× slower than L1. CPUs have physical and logical cores—logical cores share parts like FPUs but not ALUs, akin to chefs sharing a kitchen. A cache line is 64 bytes (e.g., 8 ints), so storing data contiguously improves access speed.\nHow your code accesses memory (striding) matters: unit stride (e.g., arrays) is fastest, constant stride is predictable but slower, and non-unit (e.g., linked lists) is worst. Caches are partitioned; for example, a 2-way set-associative cache with 512 blocks has 256 sets. Placement is based on memory address, with a set index, tag bits, and block offset determining cache behavior. Poor striding can lead to only one set being used, harming performance through conflict misses.\n92. Writing concurrent code that leads to false sharing\nWhen two goroutines access variables in the same cache line and at least one writes, it causes false sharing. Cores mistakenly think they’re sharing data, leading to unnecessary cache invalidation and coherence traffic, reducing performance even though no logical sharing exists.\n93. Not taking into account instruction level parallelism\nInstruction Level Parallelism (ILP) allows CPUs to execute independent instructions in parallel. Hazards occur when instructions depend on each other—like C = A + B; E = C + D—which must execute sequentially. Structuring code to reduce such dependencies allows the CPU to better utilize ILP.\n94. Not being aware of data alignment\nA word is 8 bytes, and CPUs prefer aligned access to word-sized chunks. Compilers insert padding to maintain alignment. For example, a 1-byte variable followed by an 8-byte one results in 7 bytes of padding. Organize struct fields to minimize padding—this optimizes for cache locality and GC efficiency.\n95. Not understanding stack vs heap\nThe stack allocates memory at compile time and is goroutine-local. If a function returns a local variable’s address, the variable escapes to the heap. The compiler uses escape analysis to decide where to allocate. “Sharing down” (passing to children) usually stays on the stack, while “sharing up” (returned or referenced after) forces heap allocation.\n96. Not knowing how to reduce allocations\nUse -gcflags to inspect escape analysis decisions. Design APIs to favor sharing down. For example, io.Read lets the caller reuse buffers, avoiding heap allocations. Also, inlining simple expressions helps avoid heap allocations—like string(foo) used inline. Use sync.Pool to reuse objects across goroutines when allocation cost is high.\n97. Not relying on inlining\nGo inlines functions that fall below a complexity threshold. Mid-stack inlining enables the compiler to optimize deeper call stacks. Inlining removes call overhead and can improve escape analysis. You can improve performance by isolating slow paths into separate functions and letting hot paths benefit from inlining.\n98. Not using Go diagnostic tools\n$ go tool pprof -http=:8080 &lt;file&gt;\n\n99. Not understanding how the garbage collector works\nThe garbage collector runs concurrently with the application and causes some”stop the world” moments that will impact the performance of your code.\nUsually the GC will run every time a certain heap threshold is reached.  GOGC is set to 100, which is 128 MB, and it can be updated. If your application has high spikes of traffic, it might be worth messing with this value."},"books/operating-systems-three-easy-pieces":{"slug":"books/operating-systems-three-easy-pieces","filePath":"books/operating-systems-three-easy-pieces.md","title":"Operating Systems, Three Easy Pieces","links":[],"tags":[],"content":"Coming soon!"},"posts/runner-orchestration-in-controller-runtime":{"slug":"posts/runner-orchestration-in-controller-runtime","filePath":"posts/runner-orchestration-in-controller-runtime.md","title":"Controller Orchestration Model in Controller Runtime Library","links":[],"tags":[],"content":"The controller-runtime library is utilized by numerous controller builders and templating engines (Kubebuilder, Operator SDK to name a few) to standardize the management of Kubernetes controllers. It handles the lifecycle of these controllers as well as webhooks, caches, servers and more while offering a fairly simple interface to build your operator on top of through it’s Reconciler.\nI’ve been curious about how it works internally and decided to start diving into how the Manager handles the life-cycle of all these runnables.\nTo clarify,Runnable is just a simple interface with a Start function which controllers, webhooks, caches and more all implement.\ntype Runnable interface {\n\tStart(context.Context) error\n}\nDigging a bit deeper, you’ll notice that the manager splits these runnables by functionality into runnableGroups, so that like-minded objects can be added, reconciled, and shutdown together. For instance, the way webhooks are handled internally is completely different from a leader elected controller, yet the life-cycle can be handled the same—they all need to start and stop.\nEach runnable spins off into it’s own goroutine so that all of the runnables can do their jobs at the same time. The question then becomes, how exactly does the manager effectively orchestrate the lifetime of these runnables? Let’s take a closer look:\nStep by Step\n\n\nThe Start function starts by using first encapsulating the logic in a sync.Once callback called startOnce, to ensure the inside only runs one time.\nfunc (r *runnableGroup) Start(ctx context.Context) error {\n\tvar retErr error\n \n\tr.startOnce.Do(func() {\n        ...\n\n\nWe kick off a goroutine withr.reconcile . This is the internal reconciler that initiates all of the runnables. You’ll see very shortly how it works.\ngo r.reconcile()\n\n\nWe attain the lock, mark the group as started and mark each runnable in the group as ready to start (Keep signalReady in mind, it’ll come up later) and add it to the runnable dispatch channel simply called ch.\nr.start.Lock()\nr.started = true\nfor _, rn := range r.startQueue {\n\trn.signalReady = true\n\tr.ch &lt;- rn\n}\nr.start.Unlock()\n\n\nIf there is nothing in the start queue, we return from Start. There’s nothing to run, so there’s nothing to do.\nif len(r.startQueue) == 0 {\n\treturn\n}\n\n\nThis next section involves coordination with the reconcile method which we recently started in it’s own goroutine in step 2, so let’s look at that method first. We start by reading off of the dispatch channel ch which we filled in step 3.\nfunc (r *runnableGroup) reconcile() {\n\tfor runnable := range r.ch {\n\t\t...\n\n\nThe first thing we do in the loop is very important. Part of the interesting bit to this logic is that the manager can support adding new runnables after having already started. With that in mind, we only want to add the runnable to the wait group if the manager is not in shutdown sequence. The reason is that executing wg.Add after wg.Wait is called will cause the program to panic.\nwg.Wait is a blocking call that waits for the wait group to decrement back to 0, so calling wg.Add after doesn’t make any sense—hence the panic.\nSo in the chance we are in shutdown, we continue and avoid adding the runner.\n{\n\tr.stop.RLock()\n\tif r.stopped {\n\t\tr.errChan &lt;- errRunnableGroupStopped\n\t\tr.stop.RUnlock()\n\t\tcontinue\n\t}\n\tr.wg.Add(1)\n\tr.stop.RUnlock()\n}\n\n\nThis is the portion that actually starts the individual runnable. There are some important aspects to look at. First, you’ll notice this whole section is nested in a goroutine, that’s because each runnable runs in parallel.\nAnother interesting bit is the nested goroutine. This block acts as a signaler back to the Start method, telling it “Hey, this runnable has started.” It’s put it it’s own thread so that it’s non-blocking to the actual start of the runnable. It signals by sending the runnable into the startReadyCh which the Start method is ready to receive from.\nLastly, we make sure to defer wg.Done() to ensure that this runnable is correctly checked off in the shutdown process (decrementing that counter we spoke of previously).\ngo func(rn *readyRunnable) {\n\t// Signal back to Start method\n\tgo func() {\n\t\tif rn.Check(r.ctx) {\n\t\t\tif rn.signalReady {\n\t\t\t\tr.startReadyCh &lt;- rn\n\t\t\t}\n\t\t}\n\t}()\n \n\tdefer r.wg.Done()\n \n\t// Start the runnable\n\tif err := rn.Start(r.ctx); err != nil {\n\t\tr.errChan &lt;- err\n\t}\n}(runnable)\n\n\nGoing back to the Start method, we can see the relationship it has with reconcile.\nfor {\n\tselect {\n\tcase &lt;-ctx.Done():\n\t\tif err := ctx.Err(); !errors.Is(err, context.Canceled) {\n\t\t\tretErr = err\n\t\t}\n\t// Remove the runnable from the queue\n\tcase rn := &lt;-r.startReadyCh:\n\t\tfor i, existing := range r.startQueue {\n\t\t\tif existing == rn {\n\t\t\t\tr.startQueue = append(r.startQueue[:i], r.startQueue[i+1:]...)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif len(r.startQueue) == 0 {\n\t\t\treturn\n\t\t}\n\t}\n}\nWe read runnables off of the startReadyCh channel which sent the runnable down the pipe in step 7. We find it and eliminate it from the startQueue. Once the start queue is empty, we can finally quit out of the Start method because we’ve successfully launched all of our runnables!\nNow, let’s now see how the shutdown process handles these runnables when we decide to stop the manager.\n\n\nShutdown is signaled through the StopAndWait method. Once again we use sync.Once to execute the section exactly once.\nfunc (r *runnableGroup) StopAndWait(ctx context.Context) {\n\tr.stopOnce.Do(func() {\n\t\t...\n\n\nWe first defer the closing of the dispatch ch channel, we don’t want any more runnables being added and spun up during the termination process.\ndefer func() {\n\tr.stop.Lock()\n\tclose(r.ch)\n\tr.stop.Unlock()\n}()\n\n\nNext we internally call Start, this might seem backwards at first, but we trigger it here to ensure we kick off the reconcile loop and consume the r.ch channel. We want to clear any runnables in the queue and ensure the wait group is back to 0.\n_ = r.Start(ctx)\nr.stop.Lock()\n// Store the stopped variable so we don&#039;t accept any new\n// runnables for the time being.\nr.stopped = true\nr.stop.Unlock()\n\n\nThe context is passed into all the runnables in step 7 line 12, so by calling cancel here, we are effectively signaling to all the runnables, “The shutdown process is in effect, please stop your reconcile loops”, which causes those runnable loops to break and exit.\n// Cancel the internal channel.\nr.cancel()\n\n\nHere we have an interesting concurrency pattern at play. This pattern let’s us either wait for the wait group to complete, i.e, reach 0 or wait for a set timer to expire. This pattern is ideal because it achieves flexibility by creating multiple exit conditions.\ndone := make(chan struct{})\ngo func() {\n\tdefer close(done)\n\tr.wg.Wait()\n}()\n \nselect {\ncase &lt;-done:\n\t// We&#039;re done, exit.\ncase &lt;-ctx.Done():\n\t// Calling context has expired, exit.\n}\n\n\nAnd there you have it! The complete controller lifecycle orchestrated by the manager in controller-runtime. Feel free to view the code in it’s entirety here."},"books/effective-typescript":{"slug":"books/effective-typescript","filePath":"books/effective-typescript.md","title":"Effective TypeScript","links":[],"tags":[],"content":"Abstractions and Type Hierarchies\nPrefer relying on abstractions rather than concrete classes. Think in terms of sets and subsets: &lt;T extends Foo&gt; means T is a subset of Foo, just like subclasses are more specific versions of their superclass. Generics are essentially functions for types, and you constrain them using extends.\nType Assertions and Narrowing\nUse type assertions when you know more than TypeScript does — for instance, to convert from one type to another if either is a subset of the other, or when you’re certain a value isn’t null. Type assertions bypass excess property checks, so be intentional. Also, assertions like as const can help narrow down to literal or tuple types.\nWhen dealing with unknown, it’s not assignable to anything without a type assertion. It’s useful when you expect a value but don’t know its shape. You can narrow unknown using instanceof, &#039;key&#039; in obj, or a custom type guard (foo is SomeType).\nFunction Types and Utilities\nIf you repeat function signatures, define function types. To mimic an existing function type, use typeof fn. You can also extract a function’s return type and parameters using ReturnType&lt;typeof fn&gt; and Parameters&lt;typeof fn&gt;. Conditional types let you create functions that adapt to multiple input types and return accordingly.\nInterfaces and Objects\nInterfaces are flexible: they can be extended or even augmented (declaration merging). If you need a subset, use Pick. To handle dynamic data (like parsed CSVs), use index signatures such as [key: string]: string. For safety and clarity, annotate object literals and return types explicitly.\nLoops, Arrays, and Mutability\nFor performance, avoid for-in loops — for-of or traditional loops are faster. number[] is a subtype of readonly number[], so you can assign mutable arrays to readonly ones, but not the other way around. If a function doesn’t modify its parameters, mark them readonly.\nLiteral and Narrow Types\nWriting as const after a value tells TypeScript to infer the narrowest possible type, including tuple types for arrays. This is great for reducing unwanted widening.\nTypeScript vs JavaScript\nRemember that JavaScript’s typeof isn’t the same as TypeScript’s type system. Don’t conflate the two.\nEnums, Unions, and Access Modifiers\nPrefer unions of literal types over enums — enums transpile messily into JavaScript. When it comes to private fields, use # instead of private. It enforces privacy at runtime, not just in types.\nMiscellaneous Best Practices\nLocalize your use of any — avoid typing entire variables as any; instead, use as any in narrow contexts where you’re confident. Arrays can widen as you add elements, so be careful. Try the type-coverage package with the --detail flag for a deep dive on what’s covered.\nDestructuring Defaults\nWhen destructuring objects, you can set defaults inline: const { foo = &#039;foo&#039; } = obj.vars."},"books/apprenticeship-patterns":{"slug":"books/apprenticeship-patterns","filePath":"books/apprenticeship-patterns.md","title":"Apprenticeship Patterns","links":[],"tags":[],"content":"Start by writing down five things you don’t know. Then, use tests as a method to explore and learn a new language. Choose a primary language and deliberately push beyond your usual scope with it—don’t just stay in your comfort zone. As you do this, become idiomatic in that language; immerse yourself until your usage feels natural and native.\nAdopt a “white belt” mindset—embrace your ignorance and don’t pretend to know what you don’t. Expertise isn’t the goal; it’s a byproduct of consistent, long-term effort. Keep learning and actively seek out areas where you’re weak. Pick one specific skill, tool, or technique and dig deep to close your knowledge gaps.\nGrowth comes from discomfort. Take on jobs that scare you and push your limits. Use feedback loops to keep projects alive and evolving. One effective strategy is to reimplement something you already know well—it forces a deeper understanding.\nRemember that a craftsman’s job is to build things that serve others. It’s not about self-expression, though beauty can emerge from the work. Study the classics; they’ll give you perspective and inspiration when you hit dull stretches. Having a title means nothing—apprenticeship never truly ends.\nStrive to be the weakest person on the team. That’s where real learning happens. The fear of being rejected by a mentor is usually exaggerated—most want to help. Seek out a community that shares your values and ambitions. Don’t shy away from menial tasks on important projects; these are opportunities to prove your competence and earn trust.\nStop getting distracted by pointless things. Instead, get intimately familiar with your weaknesses. There’s a crucial balance to strike between learning and producing—too much of either can be a trap.\nBuild toy systems that reflect the real-world systems you deal with; it’ll sharpen your intuition. Read other people’s code and get good—really good—at understanding it. Pick a demanding open-source project, dissect it, learn how it’s built, and document what you discover.\nMake it a habit to revisit your old notes. The less skilled you are, the more you’ll overestimate your ability. Don’t let your team’s perception of you distort your self-awareness. And finally, understand that expertise can be a trap. True understanding comes when you reconstruct the original context in which an idea was born."},"books/kubernetes-in-action":{"slug":"books/kubernetes-in-action","filePath":"books/kubernetes-in-action.md","title":"Kubernetes in Action","links":["iptables","Reverse-Proxy","Linux-Namespace","NAT","Optimistic-Concurrency","060124---Linux-dir.d-convention","Loopback-and-Network-Interface"],"tags":[],"content":"Chapter 0 - StatefulSets\nSummary\n\nStatefulSets solve the problem of managing unique, stateful pods with stable names and persistent storage, allowing replication.\nStateless instances are replaceable like cattle, while stateful instances are unique like pets, needing consistent identity.\nHeadless services provide unique DNS entries for each pod, enabling direct connections and communication.\nPersistent volume claims ensure data persistence, preventing loss during scaling or pod deletion.\n\nProblem Statement\n\nHow do you create pods that have independent state and their own storage volumes, but can still be replicated by a replicaset?\nThere are some tricks but none of them really provide a good way of doing this in k8s.\nstateful sets fix this problem. Where instances of an application need to be treated as “non-fungible” individuals where they have a stable name and state.\n\nPets vs Cattle Analogy\n\nStateless instances are like cattle, you dont really care about them individually and you dont name them. One can die, and another can be created without issue.\nStateful instances are more important — like pets. When one dies, you can’t just replace it. In the case of apps, the new instances need to have the same name and identity as the old one.\n\nStatefulSets\n\nWhen a stateful pod instance dies, the new on will get the same name, network identity and state as the one it’s replacing.\nThey can be scaled similar to replicationcontrollers and replicasets.\nEach pod created by a statefulset gets its own set of volumes/storage.\nPods in stateful sets are given incremental index names (A-0, A-1, …) because that makes them predictable.\nThese pods need to retain their hostname as well, because you may want to work on a specific pod that has a specific state.\n\nHeadless (Governing) Service\n\nAll “headless” means is it does not have a stable IP address (cluster IP). When queried, it returns the IP addresses of the individual pods, allowing clients to connect directly to these pods.\n\nThink about it, if something is headless it means it doesn’t come through a single source.\n\n\nIn order to retain their hostnames, you need to create a governing headless service to provide the actual network identity to each pod.\nIn this service, each pod gets its own DNS entry.\nSo a pod in a statefulset might be reachable via a-0.foo.default.svc.cluster.local\nYou can look up all the pods in a statefulset via SRV records for foo.default.svc.cluster.local.\n\nPersistent Volume Claims\n\nFor every pod that statefulset creates, it also needs to create a persistent volume claim from a template that exists in the statefulset.\n\n\n\n\n\nScaling down deletes the pods, but not the claims. Because if it did, those volumes would be recycled and lost. We dont want that because if a pod gets accidentally deleted, we don’t want its data to be lost.\n\nStatefulSet Manifest\napiVersion: apps/v1beta1\nkind: StatefulSet\nmetadata:\n  name: kubia\nspec:\n  serviceName: kubia  // Name of headless service \n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia-pet\n        ports:\n        - name: http\n          containerPort: 8080\n        volumeMounts:\n        - name: data\n          mountPath: /var/data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      resources:\n        requests:\n          storage: 1Mi\n      accessModes:\n      - ReadWriteOnce\n\nvolumeClaimTemplates will be used to create a persistent volume claim for each pod. The names of the pvc for each pod become &lt;vc-template-name&gt;-&lt;pod-name&gt;\n\nSRV records\n\nThese records point to the hostnames of the pods backing the headless service.\nThis allows these stateful pods to discover and communicate with each other!\n\nHow StatefulSets Deal With Failures\n\nIt won’t create a new pod until it knows for certain the old one has stopped running.\nIf a node’s network goes down, the control plane may never know if the pod got actually deleted so you’ll have to manually delete it.\nDon’t do this unless you are 100% sure that the node is no longer running or reachable.\n\nChapter 1 - Understanding Kubernetes Internals\nSummary\n\nThe control plane consists of etcd, API server, scheduler, and controller manager, while worker nodes have kubelet, kube-proxy, and container runtime.\netcd is a fast, consistent, distributed key-value store accessed only by the API server using the RAFT algorithm.\nThe API server provides a RESTful interface for CRUD operations on etcd and handles client requests through authentication, authorization, and admission control.\nThe scheduler assigns pods to nodes based on resource availability and affinity, while kubelet deploys and manages these pods on each node.\nkube-proxy manages iptables rules for service traffic routing, and CNI plugins enable node-to-node communication.\nPods communicate through a NAT-less network set up by CNI plugins, using virtual ethernet pairs and network bridges on each node.\n\nArchitecture\n\nOn control plane:\n\netcd\nAPI server\nscheduler\ncontroller manager\n\n\nOn worker nodes:\n\nkubelet\nkubernetes service proxy\ncontainer runtime\n\n\nCommunication by components and api server are usually initiated by the components.\nMost of the resources (expect for kubelet) run as pods in the kube-system namespace.\n\n\n\n\netcd\n\nfast consistent, distributed, key-value store.\nThe API server is the only thing that talks to etcd.\nkeys in etcd look like directories, but they are just strings really. like /registry/configmaps/...\nUses RAFT algorithm to achieve consistent state.\n\nRAFT algorithm\n\nMajority rules. If the cluster goes into a “split-brain” situation, the split with more etcd instances will have the true state.\nUsually there is one “leader” that handles requests and coordinates updates to cluster state.\nThe side with minority becomes read only until all instances are synced back up to a new state.\n\nAPI Server\n\n\nA RESTful API that can perform CRUD operations on the etcd.\n\n\nAn interface for querying and modifying the cluster state.\n\n\nkubectl is one of the clients we are familiar with using to communicate to the apiserver.\n\n\nA client request goes through 4 main stages.\n\nAuthentication: Figures out who you are.\nAuthorization: Figures out if you can do what you want.\nAdmission Control: Modify resources for different reasons, like init missing fields or overriding them.\n\n\n\nThe API server is observed by other components. So changes to a resource by the API server may alert another resource that is interested in that change.\n\n\nFor instance, if a resource is created, changed or deleted, the control plane component is notified.\n\n\nEvery time an object is updated, the server sends the new version of the object to all connected clients that are subscribed to it.\n\n\nScheduler\n\nAll the scheduler does is update the pod definition through the API server.\nThe kubelet (who watches the API server for updates) is the one who actually deploys the pod.\nIt first finds acceptable pods by looking at hardware resources, pod affinity and more.\nIt then picks the best option from the bunch.\n\nControllers running in the Controller Manager\n\nThe controller manager is a single program that runs each of these controllers as goroutines.\nThe controllers do the job of converging to the desired state as specified in the resources deployed through the API server.\nThere is a controller for each resource, to basically manage it.\n★ “Resources are descriptions of what should be running in the cluster, Controllers are the active kubernetes components that perform the actual work as a result of the deployed resources.”\nEach controller subscribes to the API server for the resources they are responsible for. (Using “watch” mechanism).\nKeep in mind that controllers post the api server the same way a user would.\n\nControllers\n\nReplicaSet Controller: It sees that the desired pod state is not met, it creates new Pod manifests and posts them to the API server. Then it lets the scheduler and kubelet do their jobs.\nDeployment Controller: It works by creating or removing replicasets.\nNamespace Controller: When a namespace is deleted, all the resources are also deleted.\nEndpoints Controller: Keeps the endpoints list constantly updated with the IPs and ports of pods matching the label selector.\n\nKubelet\n\nRegisters the node its running on by creating a Node resource.\nMonitors the API server for pods that have been scheduled to its node.\nIt then starts those Pod’s containers.\nIt also monitors all the containers and reports their status to the api server.\nKubelet is the thing running the container liveness probes and restarts then when they fail.\nIt can also run containerized versions of the control plane.\n\nkube-proxy\n\nOne kube-proxy runs on each node.\nThe kube-proxy watches for changes to services using the api server’s watch mechanism and updates the iptables rules depending on the ips of the pods.\niptables perform the actual load-balancing.\n\nHow DNS server works\n\nAll pods use the cluster’s internal DNS by default.\nkube-dns pod watches for changes to services and endpoints and updates the dns records with every change.\n\nHow Ingress controllers work\n\nIts a Reverse Proxy.\nWatches for changes in ingress, services, and endpoints resources and updates the config of the proxy server (e.g. nginx) accordingly.\nIt does not involve iptables, the reverse proxy handles the traffic in this case.\nIt routes directly to the Pods and does not go through services at all.\n\nEvent resource\n\nThe control plane and kubelet emit Events.\nkubectl get events --watch to retrieve events as they occur.\n\nUnderstanding what a running Pod is\n\nA “pause” container is created when a Pod is created on a Node.\nThis container is what holds all the containers of a Pod together!\nIt serves as an infrastructure container so that the containers of a Pod share the same network and linux namespace.\nThe lifecycle of this Pod is tied to that of the other containers in the Pod.\n\nInter-pod networking\n\nIt’s NAT-less and each Pod gets its own unique IP address.\nThis is setup by a Container Network Interface (CNI) plugin.\nFor communication between Pods to work, the IPs must remain consistent, hence the NAT-less.\nPods talking to outside services does have NAT.\nPods on a node are connected to the same bridge through virtual ethernet interface pairs.\nOne interface of the pair remains in the host’s namespace (node) listed vethXXXX. The other is moved into the “pause” container’s networking namespace and renamed eth0.\nThese work like two ends of a pipe.\nThe interface of the host’s network namespace is attached to a network bridge.\nSo if pod A sends a network packet to pod B, the packet first goes through pod A’s veth pair to the bridge and then through pod B’s veth pair. All containers on a node are connected to the same bridge, which means they can all communicate with each other.\nFor each pod on a node, there is one network interface (the host end of the veth pair) in the host’s network namespace that connects to the network bridge.\nConnecting between nodes means connecting the node bridge to the physical network adapter of the node.\nThen the connection travels “over the wire” to the other node.\nIt’s also worth noting that each node has a certain IP range for its pods so that there is no overlap.\n\nContainer Network Interface (CNI)\n\nThey are the layer of network that allows for node-to-node communication.\n\nHow Services are implemented\n\nThe service IP isn’t real. Its just used by the iptables to know “when you see IP A, re-route that to pod IPs B-Z”. That’s why it isn’t pingable.\nkube-proxy uses iptables rules to redirect packets destined for the service IP and reroute them to one of the pod ips backing the service.\n\nRunning highly available clusters\n\nLeader election means that one of them is doing stuff while the others are inactive.\nIt can also mean that one is performing the writes while the others are readonly.\nIts normal to have multiple instances of the control plane, with only one being active.\nThey choose who is leader by racing to see who is the first to put that title in their metadata.annotations.\nIt uses optimistic concurrency to do this safely.\n\nChapter 5 - Services\nSummary\n\nServices provide load-balanced communication to pods based on labels, accessible within or outside the cluster.\nUse Endpoints to connect to external services and Ingress for exposing services to external clients with path-based routing.\nReadiness probes ensure a pod is ready to handle requests before traffic is sent to it.\nHeadless Services, with clusterIP set to None, directly expose pod IPs without an assigned service IP.\n\nCreating Services\n\nAllow you a load-balanced way to communicate to all pods of a certain label.\nUse label selectors to decide which pods are part of the service.\nCLUSTER-IP means it’s only accessible within the cluster, not outside.\nServices are meant to expose a group of pods to another group of pods within the cluster and outside.\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubia\nspec:\n  ports:\n    - name: http\n      port: 80           // Port this service is available on\n      targetPort: 8080   // Container port the service will forward to\n    - name: https\n      port: 443          // Port this service is available on\n      targetPort: 8443   // Container port the service will forward to\n      \n  selector:\n    app: kubia  // All pods with app=kubia will be part of service\n\nYou can hit the service from a pod using exec:\n\nkubectl exec &lt;pod name&gt; -- curl -s http://&lt;SERVICE IP&gt;\n\nIf you specify port names of the Pod manifest, then you can reference those port names  in the service. This is helpful so that when you change one number, you dont need to change the other.\nYou can discover service IPs through env variables, DNS, or FQDN.\n\nConnecting to Services via FQDN\nbackend-database.default.svc.cluster.local\n\n\nbackend-database is the service name.\ndefault is the namespace.\nThese are the only two things that are necessary to hit the service.\n\nkubectl exec -it &lt;pod name&gt; bash\n...\nroot@kubia-3inly:/# curl kubia.default\n&gt; You’ve hit kubia-3inly\n\nConnecting to Services outside of the cluster\n\nYou can create an Endpoints resource where you establish a set of IPs to hit.\nCouple that with a service resource to talk to an outside service.\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-service\nspec:\n  ports:\n    - port: 80\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: external-service\nspec:\n  type: ExternalName\n  externalName: someapi.somecompany.com\n  ports:\n    - port: 80\nsubsets:\n  - addresses: \n    - ip: 11.11.11.11\n    - ip: 22.22.22.22\n\nYou can use direct IP addresses or FQDN of the service.\n\nExposing using NodePort\n\nNodePort allows you to expose an app using the nodes ip by assigning the port for that app. All nodes on the cluster will make sure that the given port will route to that app.\nSo you can have multiple apps using the NodePort as long as they are using different port numbers.\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: biggorilla-service\nspec:\n  type: NodePort\n  selector:\n    app: biggorilla\n  ports:\n    - protocol: TCP\n      port: 80            // Port of services interal cluster ip\n      targetPort: 3000    // Target port of backing pod\n      nodePort: 30123     // Service will be accessing through this port\nExposing services to external clients using Ingress\n\n\nYou can make services accessible externally using NodePort, LoadBalancer or Ingress.\n\n\nA NodePort service makes the service accessible from outside the cluster.\n\n\nWhen a client sends a request to an Ingress, the host and path in the request determines which service the request is forwarded to.\n\n\nIngress is just a filter for the given client request.\n\n\nNotice how we can configure different hosts as well as different paths.\n\n\nYou can also enable TLS (Transport Layer Security), which encrypts communication from the client to the ingress controller (Page 179).\n\n\napiVersion: v1\nkind: Ingress\nmetadata:\n  name: kubia\nspec:\n  rules:\n    - host: kubia.example.com\n      http:\n        paths:\n          - path: /kubia\n            backend:\n              serviceName: kubia-nodeport\n              servicePort: 80\n\t\t  - path: /foo\n            backend:\n              serviceName: foo\n              servicePort: 80\n    - host: foo.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: bar\n              servicePort: 80\n            \nReadiness Probes\n\nProbe a pod by sending a request to it and seeing the response.\nWhen the response is successful, we can start sending client requests to that pod.\nreadinessProbe is added to the Pod manifest.\n\nCreating a Headless Service\n\nSetting the clusterIP field to None makes a Service headless.\nThis means you k8s won’t assign an IP to the service.\nThis will give you the Pod IP’s directly.\n\nChapter 6: Volumes\nSummary\n\nVolumes allow containers within a Pod to share data, and they are ephemeral.\nemptyDir volumes provide a shared empty directory for containers.\ngitRepo volumes clone a git repository into an emptyDir.\nPersistentVolumes (PVs) and PersistentVolumeClaims (PVCs) provide persistent storage, decoupling storage from pods.\n\nIntroducing Volumes\n\nVolumes all containers within the same Pod to share data.\nEach container can mount the volume in any location in their filesystem.\nVolumes are ephemeral — they will be lost when the Pod restarts.\n\nUsing emptyDir\n\nUsed to share data between containers.\nLiterally just creates an empty directory which containers can read/write to.\nIn the example below, the volume is being shared between the two containers. Notice how they are mounting it at different locations.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune\nspec:\n  containers:\n\t- image: luksa/fortune\n\t  name: html-generator\n\t  volumeMounts:\n\t  - name: html\n\t    mountPath: /var/htdocs\n\t- image: nginx:alpine\n\t  name: web-server\n\t  volumeMounts:\n\t  - name: html\n\t\tmountPath: /usr/share/nginx/html\n\t    readOnly: true\n\t  ports:\n\t  - containerPort: 80\n\t    protocol: TCP\n\tvolumes:\n\t- name: html\n\t  emptyDir: {}\nUsing gitRepo\n\nIt just emptyDir but then a git repo is cloned into that directory.\nCan use this to serve the latest version of your git repo.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gitrepo\nspec:\n  containers:\n\t- image: nginx:alpine\n\t  name: web-server\n\t  volumeMounts:\n\t  - name: html\n\t\tmountPath: /usr/share/nginx/html\n\t    readOnly: true\n\t  ports:\n\t  - containerPort: 80\n\t    protocol: TCP\n\tvolumes:\n\t- name: html\n\t  gitRepo:\n\t    repository: github.com/maxcelant/pylox.git\n\t    revision: master\n\t    directory: .      // Clone into the root dir of the volume\nUsing hostPath\n\nThis type of volume allows you to point to a node’s filesystem.\nDon’t store Pod specific data on a node though, because if that pod changes nodes, that will be lost.\nOnly good for persistent storage on a one node cluster like Minikube.\n\n\n\n\nPersistentVolumes and PersistentVolumeClaims\n\nYou create a PersistentVolume resource where you specify the size and access it supports.\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mongodb-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n    - ReadOnlyMany\n  persistentVolumeClaimPolicy: Retain\n  gcePersistentDisk:\n    pdName: mongodb\n    fsType: ext4\n\nBy setting the persistentVolumeClaimPolicy: Retain, that means that even after this volume detaches from the Pod, itll still hold that data.\nUser submits a PersistentVolumeClaim resource where they configure how much capacity and what access rights to they need and k8s will bind a PersistentVolume to the Pod based on that claim!\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongodb-pvc\nspec:\n  resource:\n    requests:\n      storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n\nPersistentVolumes, like cluster nodes, dont belong to any specific namespace.\nIn your pod manifest, you reference the PVC not the PV directly, decoupling the storage from the requester.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune\nspec:\n  containers:\n\t- image: mongodb\n\t  name: mongodb\n\t  volumeMounts:\n\t  - name: mongodb-data\n\t\tmountPath: /data/db\n\t  ports:\n\t  - containerPort: 27017\n\t    protocol: TCP\n\tvolumes:\n\t- name: mongodb-data\n\t  persistentVolumeClaim:\n\t    claimName: mongodb-pvc   // Reference the claim!\nChapter 7: ConfigMaps\nSummary\n\nKubernetes uses ConfigMaps and Secrets to securely manage and decouple application configurations from container images.\nEmbedding configs in Docker images is insecure; Kubernetes overcomes this with dynamic config management.\nENTRYPOINT and CMD define container behavior, while environment variables and command-line arguments can be injected into pods.\nSecrets manage sensitive information securely, and Kubernetes ensures atomic updates to config files.\n\nProblem with Docker and Configs\n\nUsing configuration files in Docker containers is tricky because you would have to “bake it” into the container image.\nThis is roughly equivalent to hard-coding config values into the app.\nThis is also insecure.\n\nENTRYPOINT and CMD\n\nENTRYPOINT defines the executable invoked when the container is started.\nCMD specifies the args that get passed to the ENTRYPOINT.\nUse exec form for ENTRYPOINT\n\nCommand and arguments in K8s\n\nYou can also run your pods and inject command line arguments into them with the Pod yaml.\n\nkind: Pod\nspec:\n  containers:\n  - image: some/image\n\tcommand: [&quot;/bin/command&quot;]\n\targs: [&quot;arg1&quot;, &quot;arg2&quot;, &quot;arg3&quot;]\nEnvironment variables in k8s\n\nSetting env vars is done in the container definition\n\nkind: Pod\nspec:\n containers:\n - image: luksa/fortune:env\n   env:\n   - name: INTERVAL\nvalue: &quot;30&quot;\n   name: html-generator\nConfigMaps purpose\n\nWe want to decouple the app’s config from the app itself. Because those options can change!\nConfigMap is a map of key-value pairs. These contents are passed to the containers as environment variables or as files in a volume.\nYou can have ConfigMap’s with the same name, living in the different namespaces (dev, prod).\nThe actual contents of the ConfigMaps will be different (because its different namespaces) but this allows you to use the same name in the Pod specs of each namespace.\n\nCreating a ConfigMap\napiVersion: v1\ndata:\n  sleep-interval: &quot;25&quot;\nkind: ConfigMap\nmetadata:\n  creationTimestamp: 2016-08-11T20:31:08Z\n  name: fortune-config\n  namespace: default\n  resourceVersion: &quot;910025&quot;\n  selfLink: /api/v1/namespaces/default/configmaps/fortune-config\n  uid: 88c4167e-6002-11e6-a50d-42010af00237\n\nYour sources can be anything from a json file to a whole directory.\n\n$ kubectl create configmap my-config\n\t--from-file=foo.json  \n\t--from-file=bar=foobar.conf  \n\t--from-file=config-opts/\n\t--from-literal=some=thing\nUsing env variables from a ConfigMap\n\nThe environment variables is called INTERVAL.\nWe reference the key sleep-interval.\n\nmetadata:\n  name: fortune-env-from-configmap\nspec:\n  containers:\n  - image: luksa/fortune:env\n    env:\n    - name: INTERVAL\n\t  valueFrom:\n\t\tconfigMapKeyRef:\n\t\t  name: fortune-config\n\t\t  key: sleep-interval\n\nYou can also expose all the keys from a ConfigMap using a prefix.\n\nspec:\n  containers:\n  - image: some-image\n    envFrom:\n    - prefix: CONFIG_\n      configMapRef:\n        name: my-config-map\n\nSo if it had FOO and BAR in the config map, that would be CONFIG_FOO and CONFIG_BAR.\nYou can also pass config map keys as command-line arguments.\n\nUsing a ConfigMap volumes\n\nMostly used for larger config files.\nThis gives you the ability to update the config without recreating the pod or restarting the container.\nAnything in the configmap-files directory will be added to the configmap.\n\n$ kubectl create configmap fortune-config \\\n\t\t\t--from-file=configmap-files\n \nconfigmap &quot;fortune-config&quot; created\n\nYou can also mount volumes with only a portion of the key-values in a configmap.\n\nvolumes:\n- name: config\n  configMap:\n    name: fortune-config\n    items:\n    - key: my-nginx-config.conf // Which keys to include\n      path: gzip.conf           // Entrys value stored here\nHow Mounting Directories Works\n\nWhen you mount a volume at a certain directory, the original content at the position will be overridden by the mounted volume.\nSo make sure you put it in a place that doesn’t already have important info!\nYou can use subPath to mount it there without affecting the pre-existing data there.\n\nspec:\n  containers:\n  - image: some/image\n    volumeMounts:\n    - name: myvolume\n      mountPath: /etc/someconfig.conf // Only mounting a file\n      subPath: myconfig.conf   // Only mounting this entry\nAtomically updating the config\n\nWhat happens if the app detects a config file change and reloads before k8s is done updating all the files in the config volume?\nWhen the configmap is updated, k8s creates a dir, writes all the files to it and the relinks a ..data symbolic link to point to this new dir.\nDoing this effectively atomically updates all of them at once.\nUpdating a config map mounted at a volume does not happen synchronously across all instances. It can take up to a whole minute for pods to sync.\n\nUsing Secrets resource\n\nSecrets are encoded in base64. This allows it contain binary values, not just plain-text.\nYou can expose secrets as env variables similar to configmaps, but this is not recommended because env variables are usually dumped in error reports.\nIn general, use secret volumes.\nWhen a secret volume is exposed to a container (or as a env variable), it is decoded and written to the file in actual form.\n\nPod Yaml Definiton Breakdown\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune-https\nspec:\n  containers:\n    - image: luksa/fortune:env\n      name: html-generator\n      env:\n        - name: INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: fortune-config\n              key: sleep-interval\n      volumeMounts:\n        - name: html\n          mountPath: /var/htdocs\n    - image: nginx:alpine\n      name: web-server\n      volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n          readOnly: true\n        - name: config\n          mountPath: /etc/nginx/conf.d\n          readOnly: true\n        - name: certs\n          mountPath: /etc/nginx/certs/\n          readOnly: true\n  ports:\n    - containerPort: 80\n    - containerPort: 443\n  volumes:\n    - name: html\n      emptyDir: {}\n    - name: config\n      configMap:\n        name: fortune-config\n        items:\n          - key: my-nginx-config.conf\n            path: https.conf\n    - name: certs\n      secret:\n        secretName: fortune-https\n\nIt mounts three volumes: html, config, and certs.\nThe config volume is mapped to a configMap called fortune-config\n\nFrom that, we want the my-nginx-config.conf key.\nThis will be available at /etc/nginx/conf.d/https.conf.\nMore info on the conf.d convention here.\n\n\nThe certs volume is populated from the data in fortune-https.\n\nThis data is mounted at /etc/nginx/certs/\n\n\n\nImage Pull Secrets\n\nWhen pulling images from a private docker hub repo, you need to specify imagePullSecrets.\n\nChapter 8: Accessing Pod Metadata\nSummary\n\nYou can use the downwardAPI to get metadata about the Pod’s environment through a volume or environment vars.’\nYou can speak to the Kubernetes API from a Pod through a proxy.\nYou can also authenticate and/or use an ambassador to communicate with it.\nFor more complex tasks, you can use a dedicated kubernetes client.\n\nIntro to the Downward API\n\nThe downward api allows us to send metadata about the pod and its environment through env variables or files (using downwardAPI volume).\nThe downwardAPI is not a REST endpoint that can be hit. Its just metadata about the environment which can be reached using env variables or volumes.\n\nExposing metadata through env variables\n\nThese are just a few things that can be looked at from the downwardAPI.\nannotations and labels cannot be exposed through environment variables because they can’t be changed while the pod is alive.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: downward\nspec:\n  containers:\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    - name: NODE_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: spec.nodeName\nExposing metadata through volume\n\npath refers to the file name in which the metadata will be stored.\nIf you want to expose as containers resource field, then you need to specify the name of that container.\n\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n    volumeMounts:\n    - name: downward\n      mountPath: /etc/downward\n  volumes:\n  - name: downward\n    downwardAPI:\n      items:\n      - path: &quot;podName&quot;\n        fieldRef:\n          fieldPath: metadata.name\n      - path: &quot;podNamespace&quot;\n        fieldRef:\n          fieldPath: metadata.namespace\n      - path: &quot;labels&quot;\n        fieldRef:\n          fieldPath: metadata.labels\n      - path: &quot;annotations&quot;\n        fieldRef:\n          fieldPath: metadata.annotations\n      - path: &quot;containerCpuRequestMilliCores&quot;\n        resourceFieldRef:\n          containerName: main\n          resource: requests.cpu\n          divisor: 1m\n      - path: &quot;containerMemoryLimitBytes&quot;\n        resourceFieldRef:\n          containerName: main\n          resource: limits.memory\n \nKubernetes API\n\nWhen you need metadata other than that of the Pod, you will need to use the Kubernetes API.\nYou need to use a proxy to talk to the k8s API.\nIt allows you to learn about how resources can be used.\n\n$ curl http://localhost:8001/apis/batch/v1\n\nFor instance this can show you information about batch/v1. Giving information about Jobs like what operations can be done on that resource, etc.\n\n$ curl http://localhost:8001/apis/batch/v1/jobs\n\nGoing further, you can look at the jobs running in your cluster.\nYou can also look at a specific job (or any resource) if you have it’s name.\nYou can get the exact same info by doing: kubectl get job my-job -o json\n\nTalking to API Server from a Pod\n\nFor a Pod to talk to the API server, you need to authenticate the communication.\nTo find the k8s API you can do kubectl get svc and look for the kubernetes service.\nYou can also do grep the KUBERNETES_SERVICE from within the pod in the env folder.\n\nroot@curl:/# env | grep KUBERNETES_SERVICE\n\nKUBERNETES_SERVICE_PORT=443\nKUBERNETES_SERVICE_HOST=10.0.0.1\nKUBERNETES_SERVICE_PORT_HTTPS=443\n\n\nYou can also point to curl https://kubernetes:443\nTo verify you are talking to the API server, you need to check if the server’s cert is signed by the CA.\nThe ca.cert, token and namespace can all be found in a secret volume attached to every Pod.\nnamespace contains the namespace that the Pod is running in.\n\nUsing an ambassador container\n\nYou can use this to communicate with the k8s API securely.\nYou run an ambassador container along side your app container and connect your app to it. Then let the ambassador talk to the API server.\nkubectl proxy binds to port 8001. Since both containers share a network interface, this works!\n\nUsing a client library\n\nIf you plan on doing more than simple API requests, its better to use a dedicated k8s API client library.\n\nChapter 9: Deployments\nSummary\n\nConcurrent updates replace all pods at once, while rolling updates switch versions gradually, both methods being prone to errors.\nUse Deployments to manage updates declaratively, ensuring smoother rollouts and rollback capabilities.\nChoose between Recreate (all at once) and RollingUpdate (one by one) for updating pods.\nParameters like maxSurge and maxUnavailable help manage the rollout rate and availability.\n\nUpdating Pods Manually — Concurrent\n\nBefore Deployments were a thing, the old way of rolling out a new container version was changing the Pod template to refer to container v2, and slowly deleting the pods of v1 and letting the ReplicaSet start up the new ones.\nThis method means deleting all the pods at once and letting the ReplicaSet create all of the new ones at the same time.\n\nUpdating Pods Manually — Rolling\n\nPerforming a manual rolling update is more difficult. It involves having to ReplicaSets and winding down the Pods in v2 while increasing those in v2.\nThis is obviously more error prone, because its more commands also your app may not support multiple concurrent version types.\n\nUpdates to Same Image Tag\n\nCurrently, if you update an image but use the same tag, it probably wont be updated because the image is cached on the node, and that will be pulled instead.\n\nRolling Updates of a ReplicationController\n\nThis method is no longer the way to do things but you use to be able to do a rolling-update using a replicationcontroller.\nkubia-v1 is the controller you want to replace.\nkubia-v2 is the name of the new one\nfoo/kubia:v2 is the image to start running in the Pods.\n\nkubectl rolling-update kubia-v1 kubia-v2 --image=foo/kubia:v2\n\nThe Pods get a new label with key deployments and some id as the value. That value is to denote which replicationcontroller is in charge of it.\n\n\n\n\n\nAll it does it starts scaling the old controller down while scaling the other up.\n\nWhy rolling-update is obsolete\n\nrolling-update actually modifies resources, which is kind of against kubernetes law in a way. Since it should be a declarative.\nAlso, this is performed by the client and not the server. So if something happens to the connection along the way, it could cause some issues.\n\nDeployments to Update Declaratively\n\nDeployment manages the replicasets, replicasets manage the pods\nThe deployment name shouldn’t have an app version, since it controls that.\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: kubia\nspec:\n  replicas: 3\n  template:\n    metadata:\n      name: kubia\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - image: luksa/kubia:v1\nname: nodejs\n\nUse --record flag when creating a deployment. It records revision history which is useful for rollbacks.\nWhen you look at the names of the pods run by a deployment, the middle hash is the replicaset that controls them. So when you do rolling updates, you’ll see those change.\n\n$ kubectl get po\nNAME                     READY     STATUS    RESTARTS   AGE\nkubia-1506449474-otnnh   1/1       Running   0          14s\nkubia-1506449474-vmn7s   1/1       Running   0          14s\nkubia-1506449474-xis6m   1/1       Running   0          14s\n\nAll you need to do is reference a new container image, and it will automatically start the update!\nkubectl set image lets you set a new container image.\nUnlike the rolling-update, this is performed by the control plane, not the client.\n\nDeployment Strategies\n\nRecreate removes all the old pods and starts all the new ones at once.\nRollingUpdate removes them one by one, keeping the app available the whole time.\n\nControlling Rate of Rollout\n\nmaxSurge means “how many MORE pods can we create than your replica count?”\n\nSo if you have total pods set to 3 and maxSurge set to 1, then at most, there will be 4 pods.\n\n\nmaxUnavailable means “how many pods can we make unavailable relative to your replica count?”\n\nSo if you have a total of 4 pods and set maxUnavailable to 1, then there will always be 3 running.\n\n\nRemember that these are both relative to your replica count.\n\n\n\n\nRolling Back\n\nThe deployment will keep the replicaset of the previous version so that if you need to perform a rollback, all you need to do is start increasing the pod count on that replicaset and decreasing the current version.\nundo command is used to abort or undo a roll out.\n\n$ kubectl rollout undo deployment kubia\n\nEach replicaset stores the complete information of the deployment at that specific revision, so don’t delete it manually.\nrevisionHistoryLimit is how many recent replicasets to keep.\nYou can also pause a rollout. This can work like a canary release. Can also allow you to make multiple changes to deployment and then kick it off.\n\n$ kubectl rollout pause deployment kubia\nSlowing Roll Outs\n\nminReadySeconds specifies how long a pod should be ready before the pod is treated as available.\nOnce the pod is available, the rollout will continue.\nYou should set minReadySeconds to a higher value, to make sure that pods are steady before continuing.\nIf a pod starts failing before minReadySeconds amount of time, it will automatically roll back.\nFor example, if you set it to 10 seconds. Then a Pod needs to be ready for at least 10 seconds\n\nChapter 14 - Managing Pods Computational Resources\nSummary\n\nYou can give max and mins for cpu and memory resources in a cluster.\nQuality of Service classes categorize your pods in a “to-kill” priority.\nLimitRange resources give resource mins and maxes for newly created pods in a namespace.\nResourceQuota set hard resource limits (as well as other limits) for the whole namespace.\nPod resource monitoring is done by cAdvisor to Heapster.\n\nRequesting Resources\n\nThe minimum amount of resources your pod needs, not the max.\nSpecified for each container in a Pod.\ncpu: 200m means 200 milicores which means 1/5 of a cpu.\n\nk exec -it requests-pod top\n\ntop can give you the CPU consumption.\nThe scheduler takes this into account when assigning a pod to a node.\nScheduling always works on how much was actually allotted not how much is currently being used.\n\nSo if a node is technically 80% full but its only really using 70% and a new pod requests at least 25%, it can’t be scheduled to that node.\n\n\nScheduler prioritizes nodes with heavy requests on them to bundle pods tightly to hopefully free up and remove unused nodes.\nIf the scheduler can’t fulfill your resource requests on any node, the pod will remain Pending.\nAny free cpu is split up accordingly between the pods on the node.\n\n\n\n\nLimiting Resources\n\nYou should limit the memory given to a container, because that cannot be taken back like the cpu can.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: limited-pod\nspec:\n  containers:\n  - image: busybox\n    command: [&quot;dd&quot;, &quot;if=/dev/zero&quot;, &quot;of=/dev/null&quot;]\n    name: main\n\tresources:\n\t  limits:\n\t\tcpu: 1\n\t\tmemory: 20Mi\n\nSum of all resource limits are allowed to exceed 100% of the nodes capacity.\nCPU is throttled, so it cannot exceed its limit.\nIf a process tries to exceed its memory allocated, it is killed and restarted.\nOOMKilled means a pod was killed because “Out of Memory”.\nYou can see why a pod was killed by doing a describe on it.\ntop command shows the memory and cpu amounts of the whole node the container is running on.\n\nQuality of Service (QoS) classes\n\nResource limits can be overcommitted. QoS decides which pod stays and which pod is killed when necessary.\nCan be found in pod yaml at status.qosClass.\nBestEffort are pods with no resource limits or requirements. They are the first killed.\nGuarenteed are pods that have limits set (or their requests match their limits)\nBurstable are pods which resource limits don’t match its requests.\nPriority is Guarenteed &gt; Burstable &gt; BestEffort\n\n\n\n\n\nTo determine which pod is killed when the pods have the same QoS, you need to look at OutOfMemory (OOM) score.\n\npercentage of available memory the process is consuming and fixed OOM score (based on pods QoS class and containers requested memory.)\n\n\nBasically the one using more memory gets killed off first.\n\nLimitRange object\n\nUsed by LimitRange Admission control plugin.\nWhen you post your pod spec, the resource is validated to make sure the pod doesn’t go above the resource limits.\nThese limits apply to all pods created in the same namespace as the limitrange resource.\nYou can also use this to set defaults for pods without explicit resource limits.\n\nResourceQuota object\n\nUsed by ResourceQuota Admission control plugin\nAllows you to specify the overall cpu and memory the pods in a namespace are allowed to consume.\n\napiVersion: v1\n\tkind: ResourceQuota\n\tmetadata:\n\t  name: cpu-and-mem\n\tspec:\n\t  hard:\n\t\trequests.cpu: 400m\n\t\trequests.memory: 200Mi\n\t\tlimits.cpu: 600m\n\t\tlimits.memory: 500Mi\n\nIt will block new pods created that will cause you to exceed these hard limits.\nIf you try to create a pod without requests or limits it will fail if you have a resource quota.\nCan also limit different resource types.\nactiveDeadlineSeconds is the amount of time a pod will live for before it is killed and restarted.\nYou can apply quotas for pods with a specific QoS.\n\nMonitoring pod usage\n\ncAdvisor on the kubelet performs a basic collection of resource consumption data.\nHeapster is a component that gathers that data for the whole cluster.\n\n$ k top node // displays cpu and memory usage\n$ k top pod --all-namespaces // actual cpu and mem usage of pods\nChapter 15: Automatic Scaling of Pods and Cluster Nodes\nSummary\n\nHorizontalPodAutoscaler is a resource that will automatically scale your pods to keep them below a resource threshold.\nVertical autoscaling is not yet available (sad).\nCluster autoscaling allows kubernetes to provision more/less nodes from the cloud provider depending on the usage.\nPodDisruptionBudget allows you to set a min and max for number of pods that are always available of a label in the cluster.\n\nHorizontal pod autoscaling\n\nPerformed by Horizontal controller which is configured by a HorizontalPodAutoscaler (HPA).\nUses heapster to get its stats.\nCalculating the required replica count is simple when it considers only a single metric.\n\nPod 1: 60% CPU utilization\nPod 2: 90% CPU utilization\nPod 3: 50% CPU utilization\nTarget Utilization: 50% CPU\n(60 + 90 + 50) / 50 = 200 / 50 = 4 (replicas)\n\nHPA only modifies on the Scale sub-resource.\nThis allows it to operate on any scalable resource (e.g. pods, deployments, statefulsets).\nAutoscaler compares a pods actual cpu consumption and its requested amount so pods will need to have that set.\nautoscale command can create a HPA quickly.\nRemember that a container’s cpu utilization is the container’s actual cpu usage divided by its requested cpu.\n\n$ k autoscale deployment kubia --cpu-percentage=30 --min=1 --max=5\nCreating a HorizontalPodAutoscaler\napiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: kubia    // Does not need to match name of deployment\nspec:\n  maxReplicas: 5\n  metrics:\n    - resource:\n        name: cpu\n        targetAverageUtilization: 30\n      type: Resource\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: extensions/v1beta1\n    kind: Deployment\n    name: kubia\nstatus:\n  currentMetrics: []\n  currentReplicas: 3\n  desiredReplicas: 0\n\nThere is a limit on how many replicas an hpa can create in a single operation.\n\nRunning commands in the cluster\n$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox\n➥ -- sh -c &quot;while true; do wget -O - -q kubia.default; done&quot;\n\nThis allows you to create an unmanaged pod on the cluster that will be deleted when you type CTRL+C.\n\nMetric Types\n\nResource: bases its autoscaling on a resource.\n\n... spec:\n  maxReplicas: 5\n  metrics:\n  - type: Resource\n\tresource:\n      name: cpu\n      targetAverageUtilization: 30\n\nPods: any metrics related to pods directly like queries per second (QPS).\n\nspec:\n  metrics:\n  - type: Pods\n\tresource:\n      metricName: qps\n      targetAverageValue: 100\n\nObject: scales pods on a metric that doesn’t pertain directly to those pods. The autoscaler obtains a single metric from the single object (e.g. Ingress).\n\n... spec:\n  metrics:\n  - type: Object\n    resource:\n      metricName: latencyMillis\n      target:\n        apiVersion: extensions/v1beta1\n        kind: Ingress\n        name: frontend\n      targetValue: 20\n  scaleTargetRef:\n    apiVersion: extensions/v1beta1\n    kind: Deployment\n    name: kubia\n...\nVertical pod autoscaling\n\nKubernetes does not currently support this.\nYou can use a Admission Control plugin called InitialResources to set usage requests for a deployment by looking at the historical usage for that deployment.\n\nCluster Autoscaler\n\nAutomatically provisions additional nodes when the existing nodes are full.\nNodes are grouped by type (called pools). So you need to specify the node type when you want an additional one.\n\n\n\n\n\nIf the cpu and memory requests on a given node are below 50%, that node is considered “unnecessary”.\nIf a system pod is running on a node, the node won’t be relinquished.\nIf a node is selected for shut down, all the pods are evicted.\n\nCordoning and draining nodes\n\nkubectl cordon &lt;node&gt; marks the node as unschedulable.\nkubectl drain &lt;node&gt; marks the node as unschedulable and then evicts all the pods from the node.\n\nPodDisruptionBudget resource\n\nAllows you to set how many pods of a certain label should always be available.\n\n$ kubectl get pdb kubia-pdb -o yaml\n \napiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: kubia-pdb\nspec:\n  minAvailable: 3\n  selector:\n    matchLabels:\n      app: kubia\nstatus:\n...\n\nthe kubectl drain command will adhere to it and will never evict a pod with the app=kubia label if that would bring the number of such pods below three.\n\nChapter 17: Best Practices for Developing Apps\nSummary\nUnderstanding the pod’s lifecycle\n\nLike tiny VMs dedicated to running only a single application.\nLocal IP and hostname will change.\nAnything written to the disk or containers filesystem will be lost when a pod is moved or killed.\nUsing volumes will persist data on container restarts (but not pod restarts).\nUsing volumes to preserve files across container restarts like this is a double-edged sword. What if the data gets corrupted and causes the newly created process to crash again?\nThe replicaset does not care if one of its pods is dead, only that the desired pod count is correct.\n\nInit containers\n\n1 or more containers that you can set to run before your main container.\nCan be used to ping the service when your app is coming up and then have it stop when the service finally up and running.\n\nspec:\n  initContainers:\n  - name: init\n    image: busybox\n    command:\n    - sh\n    - -c\n\t- &#039;while true; do echo &quot;Waiting for fortune service to come up...&quot;;\n\t  &#039;wget http://fortune -q -T 1 -O /dev/null &gt;/dev/null 2&gt;/dev/null\n\t  &#039;&amp;&amp; break; sleep 1; done; echo &quot;Service is up! Starting main&#039;\n\t  &#039;container.&quot;&#039;\n\nYou’re defining an init container, not a regular container. The init container runs a loop that runs until the fortune Service is up.\n\nPost-start hook\n\nInitiates after the app has started.\nRuns in parallel with the main process.\nUntil the hook completes, the container will be in Waiting state with reason ContainerCreating. The pod status will be Pending instead of Running.\nIf the hook fails or returns a non-zero exit code, the main container is killed.\n\nPre-stop hook\n\nExecutes immediately before a container is terminated.\nCan be used to initiate a graceful shut down.\n\n lifecycle:\n   preStop:\n     httpGet:\n        port: 8080\n        path: shutdown\n\nThis is a pre-stop hook that performs an HTTP GET request. The request is sent to http://POD_IP:8080/shutdown.\nLifecycle hooks target containers, not pods.\nYour app should react to a SIGTERM signal.\n\nShell form vs direct execution\n\nIf the SIGTERM signal isn’t hitting your app it could be because your Dockerfile is set to use the shell form (ENTRYPOINT /mybinary) instead of executing it directly (ENTRYPOINT [&quot;/mybinary&quot;]).\nThe problem is that in shell form, it creates a shell in the container then runs your app in the shell as a “child process”.\nThis means that if a signal hits the shell, it’ll have to forward it to the app.\n\nEnsuring all client requests are handled properly\n\nHave readiness probes so that k8s knows that the pod is ready to accept connections.\nWait for a few seconds, then stop accepting new connections.\nClose all keep-alive connections not in the middle of a request.\nWait for all active requests to finish.\nThen shut down completely.\nMaybe adding a pre-stop hook to sleep for 5 seconds.\n\nMaking the apps easy to run and manage\n\nDon’t use latest tag for containers.\nMake small and manageable containers.\nUse more than one label for your resources.\nAdd annotations to describe your resources.\nIn microservice architecture, pods could contain lists of names the other services the pod is using.\nUse a terminationMessagePath field in the container definition in the pod spec.\n\nIt will show up in a k describe pod to see why a pod terminated.\nYou can see the reason why the container died without having to inspect its logs.\n\n\nYou can use k logs with --previous to see the logs of the previous container.\nYou can copy logs to your local using k cp foo-pod:/var/log/foo.log foo.log\n\nBest practices for development and testing\n\nYou can connect to a backend service using those environment variables to find the service. If you need it on the outside, you can use a NodePort to connect from outside → inside.\nRun kubectl proxy on your local machine, run your app locally, and it should be ready to talk to your local kubectl proxy.\n\nUsing Minikube\n\nUse minikube mount to mount your local filesystem into the minikube vm.\nYou can copy local docker images to the minikube vm.\n\n$ docker save &lt;image&gt; | (eval $(minikube docker-env) &amp;&amp; docker load)\n\nJust make sure the imagePullPolicy in your pod spec isn’t set to Always.\n\nChapter 18: Extending Kubernetes\nCustomResourceDefinitions\n\nYou create this definition describing your custom resource.\nThen you can start creating that resource.\n\n_Instead of dealing with Deployments, Services, ConfigMaps, and the like, you’ll create and manage objects that represent whole applications or software services. A custom controller will observe those high-level objects and create low-level objects based on them._\nExample Website CRD\n\nWe want the Website resource to create a service and a pod with our app.\n\nkind: Website\nmetadata:\n  name: kubia\nspec:\n  gitRepo: github.com/luksa/kubia-website-example.git\n\nWe also need to create the crd itself\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: websites.extensions.example.com // long to prevent name clash\nspec:\n  scope: Namespaced\ngroup: extensions.example.com\nversion: v1\nnames:\n  kind: Website\n  singular: website  \n  plural: websites\n\nWebsite controller talks to the API server through a proxy (in an ambassador container).\nThe proper way to watch objects through the API server is to not only watch them, but also periodically re-list all objects in case any watch events were missed.\n\nRunning the controller as a pod\n\nYou can run it locally and use kubectl proxy as an ambassador to the api server.\nOnce its ready to deploy to prod, make it a deployment!\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: website-controller\nspec:\n  replicas: 1   // just one replica\n  template:\nmetadata:\n  name: website-controller\n  labels:\n    app: website-controller\nspec:\n  serviceAccountName: website-controller   // special service account\n  containers:\n  - name: main\n    image: luksa/website-controller\n  - name: proxy\n    image: luksa/kubectl-proxy:1.6.2   // proxy sidecar\n\nOne container runs your controller, whereas the other one is the ambassador container used for simpler communication with the API server.\n"},"books/Untitled":{"slug":"books/Untitled","filePath":"books/Untitled.md","title":"Untitled","links":[],"tags":[],"content":""}}